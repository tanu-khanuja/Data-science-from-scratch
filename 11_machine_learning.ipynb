{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d92b73-7d2c-4fd7-beb4-7e5f8bfad926",
   "metadata": {},
   "source": [
    "- Data science is mostly turning business problems into data problems and solve them \n",
    "\n",
    "- steps: collecting data - > understanding data -> cleaning data - > formatting data - > machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673cb61-5bf6-422a-94b5-ca8cc625a1f2",
   "metadata": {},
   "source": [
    "# What is a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d946e-f035-4c1b-993f-502d69e27bbc",
   "metadata": {},
   "source": [
    "Specification of a mathematical relationship that exists between different variables.\n",
    "\n",
    "Example: \n",
    "1. A business model that takes inputs like “number of users,” “ad revenue per user,” and “number of employees”\n",
    "and outputs your annual profit for the next several years - It's a simple mathematical relationships: profit is revenue minus expenses, revenue is units sold times average price, and so on.\n",
    "\n",
    "2. if you’ve ever watched poker on television, you know that each player’s “win probability” is estimated in\n",
    "real time based on a model that takes into account the cards that have been revealed so far and the distribution of cards in the deck - It's based on probability theory, the rules of poker, and some reasonably innocuous\n",
    "assumptions about the random process by which cards are dealt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc2c96-7497-48b1-8dec-331869797714",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "- ML is creating and using models that are learned from data\n",
    "\n",
    "- In other contexts this might be called **predictive modeling** or **data mining**\n",
    "\n",
    "- Our goal: create a model using existing data -> use it to find various outcomes for new data\n",
    "\n",
    "- Example: Whether an email message is spam or not?  Whether a credit card transaction is fraudulent?\n",
    "Which advertisement a shopper is most likely to click on? Which football team is going to win the Super Bowl?\n",
    "\n",
    "**Types of ML models**\n",
    "\n",
    "Machine learning models can be categorized into several types based on the learning approach and the nature of the data. Here are some common types:\n",
    "\n",
    "## Supervised Learning Models\n",
    "\n",
    "In which **there is a set of data labeled with the correct answers to learn from**.\n",
    "\n",
    "For example, if you want to teach a computer to recognize pictures of cats and dogs, you show it many pictures of cats labeled \"cat\" and many pictures of dogs labeled \"dog\". The computer learns from these examples and can then classify new pictures as either cats or dogs.\n",
    "\n",
    "Examples:\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks\n",
    "\n",
    "## Unsupervised Learning Models\n",
    "\n",
    "Unsupervised learning models are **trained on unlabeled data** and aim to find patterns, clusters, or hidden structures in the data.\n",
    "\n",
    "It's like looking at a big jumble of puzzle pieces without knowing what the picture is supposed to look like. The **computer tries to group similar pieces together** to make sense of the puzzle without any guidance.\n",
    "\n",
    "Examples:\n",
    "- K-means Clustering\n",
    "- Hierarchical Clustering\n",
    "- Principal Component Analysis (PCA)\n",
    "- Autoencoders\n",
    "\n",
    "## Reinforcement Learning Models\n",
    "\n",
    "Reinforcement learning models learn by interacting with an environment and receiving feedback in the form of rewards or penalties.(after making a series of predictions, the model gets a signal\n",
    "indicating how well it did)\n",
    "\n",
    "It's like teaching a dog new tricks. You reward it when it does something right and punish it when it does something wrong. Over time, the dog learns which actions lead to rewards and which ones don't.\n",
    "\n",
    "Examples:\n",
    "- Q-learning\n",
    "- Deep Q-Networks (DQN)\n",
    "- Policy Gradient Methods\n",
    "- Actor-Critic Methods\n",
    "\n",
    "## Semi-Supervised Learning Models\n",
    "\n",
    "Semi-supervised learning models combine supervised and unsupervised learning techniques and are useful **when only a subset of the data is labeled**.\n",
    "\n",
    "It's like having a few labeled examples along with a lot of unlabeled data. The computer tries to use the labeled examples to guide its learning process while also finding patterns in the unlabeled data.\n",
    "\n",
    "Examples:\n",
    "- Self-training\n",
    "- Co-training\n",
    "- Label propagation\n",
    "\n",
    "## Deep Learning Models\n",
    "\n",
    "Deep learning models use neural networks with many layers (deep architectures) to learn intricate patterns from data.\n",
    "\n",
    "It's like having many layers of filters to extract different features from the input data, allowing the computer to learn more intricate patterns and relationships\n",
    "\n",
    "Examples:\n",
    "- Convolutional Neural Networks (CNN)\n",
    "- Recurrent Neural Networks (RNN)\n",
    "- Long Short-Term Memory Networks (LSTM)\n",
    "- Transformer Models\n",
    "\n",
    "## Online models \n",
    "\n",
    "In which the model needs to continuously adjust to newly arriving data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c11361-904d-4054-99c3-102c2534d2d1",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "A common danger in machine learning is: \n",
    "\n",
    "**overfitting** \n",
    "\n",
    "- producing a model that performs well on the data you train it on but generalizes poorly to any new data.  \n",
    "\n",
    "- This could involve learning <u>noise</u> in the data.\n",
    "  \n",
    "- Or it could involve learning to identify <u>specific inputs</u> rather than whatever factors are actually predictive for the desired output.\n",
    "  \n",
    "**underfitting** \n",
    "\n",
    "- producing a model that doesn’t perform well even on the training data\n",
    "  \n",
    "- although typically when this happens you decide your model isn’t good enough and keep looking for a better one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917746b-d62d-430d-b223-4c73bc475df7",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"/Users/tanukhanuja/Data-science-from-scratch/mlconcepts_image5.png\" alt=\"Alt Text\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0df6c-d7d7-4e03-9bd3-f04e88a2c8d0",
   "metadata": {},
   "source": [
    "- The horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It severely underfits the training data.\n",
    "\n",
    "- The best fit degree 9 (i.e., 10-parameter) polynomial goes through every training data point exactly, but it\n",
    "very severely overfits; if we were to pick a few more data points, it would\n",
    "quite likely miss them by a lot.\n",
    "\n",
    "- And the degree 1 line strikes a nice balance; it’s pretty close to every point, and—if these data are representative—the line will likely be close to new data points as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15419a3-5d68-4784-bf61-1a6a054585a9",
   "metadata": {},
   "source": [
    "- Clearly, **models that are too complex lead to overfitting** and don’t generalize well beyond the data they were trained on.\n",
    "\n",
    "So **how do we make sure our models aren’t too complex?**\n",
    "\n",
    "- The most fundamental approach involves using different data to train the model and to test the model.\n",
    "\n",
    "- The simplest way to do this is to <u>split the dataset</u>, so that (for example) two-thirds of it is used to train the model, after which we measure the model’s performance on the remaining third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f9e092a-2d46-4ebf-a322-a7e6e97724bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import TypeVar, List, Tuple\n",
    "X = TypeVar('X') # Generic type of variable\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"\n",
    "    Split data into fractions [prob, 1 - prob]\n",
    "    \"\"\"\n",
    "    data = data[:]\n",
    "    random.shuffle(data)\n",
    "    cut = int(len(data) * prob)\n",
    "    return  data[:cut],data[cut:]\n",
    "\n",
    "random.seed(0)\n",
    "data = [n for n in range(1000)]\n",
    "train, test = split_data(data, 0.7)\n",
    "\n",
    "# Proportions should be correct\n",
    "assert len(train) == 700 \n",
    "assert len(test) == 300 \n",
    "\n",
    "# Original data should be preserved\n",
    "assert sorted(train+test) == data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dc080-8c03-4c15-8836-ebe51d76bbc9",
   "metadata": {},
   "source": [
    "- Often, we’ll have paired input variables and output variables. \n",
    "- In that case, we need to make sure to put corresponding values together in either the training data or the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41631a09-1722-4e0f-8db3-a6b50b96cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TypeVar('Y')\n",
    "\n",
    "def train_test_split(xs: List[X], ys: List[Y], test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idx, test_idx = split_data(idxs, 1-test_pct)\n",
    "    return ([xs[i] for i in train_idx],\n",
    "            [xs[i] for i in test_idx],\n",
    "            [ys[i] for i in train_idx],\n",
    "            [ys[i] for i in test_idx])\n",
    "\n",
    "# Let's check\n",
    "xs = [x for x in range(1000)]\n",
    "ys = [2*x for x in xs]\n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, 0.25)\n",
    "\n",
    "# Check if proportions are correct\n",
    "assert len(x_train) == len(y_train) == 750\n",
    "assert len(x_test) == len(y_test) == 250\n",
    "\n",
    "#check that x and y are paired correctly\n",
    "assert all(y == 2 * x for x, y in zip(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a32f03-7e6c-4916-9509-097bb98a2ee4",
   "metadata": {},
   "source": [
    "- after splitting the data into training set and test set\n",
    "- we can train our model like:\n",
    "\n",
    "`model = SomeKindOfModel()`  \n",
    "`x_train, x_test, y_train, y_test  = train_test_split(xs, ys, 0.33)`   \n",
    "`model.train(x_train, y_train)`  \n",
    "`performance = model.test(x_test, y_test)`\n",
    "\n",
    "- if model performs well on test data - model fits good\n",
    "- if model does not perform well on test data - probably overfitting\n",
    "\n",
    "However, there are a couple of ways this can go wrong. \n",
    "\n",
    "1. **The first is if there are common patterns in the test and training data that wouldn’t generalize to a larger dataset.**\n",
    "\n",
    "This scenario highlights the potential issue of overfitting in machine learning models, particularly when dealing with user activity data. If the model learns patterns specific to individual users rather than general relationships between attributes, it may not perform well on new data outside the training and test sets.\n",
    "\n",
    "For example, let's say you're predicting user behavior based on various features like time spent on a website, number of clicks, etc. **If the model learns to identify users rather than identifying general trends in user behavior**, it might not generalize well to new users or situations.\n",
    "\n",
    "To address this, it's essential to use techniques like **cross-validation, regularization, and feature engineering** to prevent overfitting and ensure that the model captures meaningful patterns in the data rather than memorizing specific instances. Additionally, **using larger and more diverse datasets can help the model learn more robust and generalizable patterns**.\n",
    "\n",
    "2. **This is a critical issue known as \"data leakage\" or \"meta-training,\" where the test set inadvertently becomes part of the training process. If you use the test/train split not just for evaluating models but also for selecting the best-performing model among several alternatives, you're effectively training on the test set.**\n",
    "\n",
    "Here's how it happens: when you try out multiple models and choose the one with the best performance on the test set, you're indirectly optimizing the model for that specific test set. As a result, the model may appear to perform well on the test set, but its performance might not generalize well to new, unseen data.\n",
    "\n",
    "To avoid this problem, it's essential to **split your data into three sets: training, validation, and test sets.** Use the training set to train different models, the validation set to evaluate their performance and select the best one, and the test set to assess the final model's performance on unseen data. This way, you can ensure that your model's performance is reliable and generalizable to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57ee54-7286-43f4-90f5-74816892a0be",
   "metadata": {},
   "source": [
    "# Correctness of Model\n",
    "\n",
    "When evaluating the performance of a binary classification model, accuracy alone may not provide a comprehensive assessment of its effectiveness. This note discusses the concept of correctness in model evaluation and illustrates it with examples.\n",
    "\n",
    "## Categories of Predictions\n",
    "\n",
    "- In binary classification, predictions fall into four categories:\n",
    "1. True Positive: The prediction is correct, and the event occurs (e.g., correctly identifying spam emails).\n",
    "2. False Positive (Type 1 Error): The prediction is incorrect, but the event doesn't occur (e.g., wrongly classifying a non-spam email as spam).\n",
    "3. False Negative (Type 2 Error): The prediction is incorrect, and the event occurs (e.g., failing to detect a spam email).\n",
    "4. True Negative: The prediction is correct, and the event doesn't occur (e.g., correctly identifying a non-spam email).\n",
    "\n",
    "## Confusion Matrix\n",
    "These categories are often visualized in a confusion matrix:\n",
    "\n",
    "| Actual              | Predicted Positive | Predicted Negative |\n",
    "|---------------------|---------------------|---------------------|\n",
    "| spam                | predicted spam (TP) | predicted not spam (TN) |\n",
    "| not spam            | predicted spam (FP) | predicted not spam (FN) |\n",
    "\n",
    "## The Leukemia Test\n",
    "- Suppose we have a test that predicts whether a newborn baby will develop leukemia with over 98% accuracy. \n",
    "\n",
    "- Surprisingly, the test is simple: it predicts leukemia if the baby's name is \"Luke,\" as it sounds similar to \"leukemia.\"\n",
    "\n",
    "- While the test achieves high accuracy, it's fundamentally flawed and serves as a cautionary example.\n",
    "\n",
    "- approximately 5 out of 1,000 babies are named Luke\n",
    "- while the lifetime prevalence of leukemia is about 1.4%.\n",
    "- Assuming these factors are independent, applying the \"Luke is for leukemia\" test to 1 million people would yield a confusion matrix.\n",
    "\n",
    "- Let's say the confusion matrix looks like: \n",
    "\n",
    "|                  | Leukemia Prediction | No Leukemia Prediction | Total  |\n",
    "|------------------|----------------------|------------------------|--------|\n",
    "| \"Luke\"           | 70                   | 4,930                  | 5,000  |\n",
    "| Not \"Luke\"       | 13,930               | 981,070                | 995,000|\n",
    "| **Total**        | **14,000**           | **986,000**            | **1,000,000** |\n",
    "\n",
    "\n",
    "- we use confusion matrix to compute statistcs of model performance\n",
    "\n",
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f102032e-9d0c-4ed0-8348-3c033ad6f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    correct_prediction = tp+tn\n",
    "    total = tp+fp+tn+fn\n",
    "    return correct_prediction/total\n",
    "\n",
    "assert accuracy(70, 4930, 13930, 981070) == 0.98114  \n",
    "\n",
    "# pretty impressive accuracy\n",
    "#But clearly this is not a goodtest, which means that we probably shouldn’t put a lot of credence in raw accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83731f7f-4502-4df9-839d-37f369f2fee5",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "**Precision** meausres <u>how accurate our positive predictions are</u> = $ \\frac{tp}{tp+fp}$ = $ \\frac{predicted\\_true\\_positives}{total\\_actual\\_positives}$\n",
    "\n",
    "**Recall** measure the <u>fraction of positives our model identifies</u> = $ \\frac{tp}{tp+fn}$ = $ \\frac{predicted\\_true\\_positive}{predicted\\_total\\_postives}$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7470eab-f48b-4a18-9b47-8ffd1e9f6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "assert precision(70, 4930, 13930, 981070) == 0.014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e8de7173-041e-4672-9023-c61bad7b3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    return tp/ (tp + fn)\n",
    "\n",
    "assert recall(70, 4930, 13930, 981070) == 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4578aa-ec1c-4ac6-bf7f-df9f9f78a19f",
   "metadata": {},
   "source": [
    "- Both are terrible numbers indicating that our model is terrible.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"/Users/tanukhanuja/Data-science-from-scratch/Precisionrecall.svg.png\" alt=\"Alt Text\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5ef81-4500-4952-8479-75dd3bfae01c",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "- It is <u>harmonic mean of precision and recall</u>, which is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1617674c-4959-4956-b6d9-252168f0c2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp: int, fp: int, fn: int, tn: int) -> float:\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2445b-11c3-4da2-b5c2-bbbc546e54c7",
   "metadata": {},
   "source": [
    "#### Chosing model using F1-score\n",
    "\n",
    "Tradeoff between precision and recall is taken into consideration.\n",
    "\n",
    "1. high F1 score with recall > precision = Confident\n",
    "2. high F1 score with precision >> recall = Extremely confident\n",
    "\n",
    "Alternatively, you can think of this as a tradeoff between false positives and false negatives. Saying “yes” too often will give you lots of false positives; saying “no” too often will give you lots of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145a1ba-c5ec-4e90-b962-78349157f4be",
   "metadata": {},
   "source": [
    "Imagine that there were 10 risk factors for leukemia, and that the more of them you had the more likely you were to develop leukemia. In that case you can imagine a continuum of tests: “predict leukemia if at least one risk factor,” “predict leukemia if at least two risk factors,” and so on. As you increase the threshold, you increase the test’s precision (since people with more risk factors are more likely to develop the disease), and you decrease the test’s recall (since fewer and fewer of the eventual disease-sufferers will meet the threshold). In cases like this, <u>choosing the right threshold is a matter of finding the right tradeoff</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83965353-89d1-47b1-8c7c-b679b96efd6c",
   "metadata": {},
   "source": [
    "# Bias-Variance tradeoff\n",
    "\n",
    "## Understanding Overfitting using bias-variance tradeoff\n",
    "- Both are metrics that assess the behavior of a model when trained on different subsets of data from the same larger population.\n",
    "\n",
    "## Bias\n",
    "- A measure of how much the model's predictions deviate from the actual values, even on the training data.\n",
    "\n",
    "- High bias typically results in underfitting.\n",
    "\n",
    "## Variance\n",
    "- A measure of how much the model's predictions vary when trained on different datasets.\n",
    "\n",
    "- High variance often leads to overfitting.\n",
    "\n",
    "## High Bias, Low Variance (Underfitting)\n",
    "- Models with high bias make significant errors on most training data, indicating an inadequate fit.\n",
    "- However, these models <u>show consistent behavior</u> across different training sets.\n",
    "\n",
    "## Low Bias, High Variance (Overfitting)\n",
    "- Models with low bias fit the training data very closely, sometimes too closely, leading to high sensitivity to training data variations.\n",
    "- This results in poor generalization to new data.\n",
    "\n",
    "## Addressing Model Problems\n",
    "- High Bias: <u> Add more features </u> to capture additional information from the data.\n",
    "\n",
    "- High Variance: <u> Reduce the number of features or obtain more data</u> to provide the model with a broader perspective.\n",
    "\n",
    "## Impact of Data Size\n",
    "\n",
    "- In Figure, we observe the effect of sample size on model fit.\n",
    "- As the dataset size increases, the degree of overfitting decreases.\n",
    "- More data helps mitigate variance but does not address bias issues caused by insufficient feature representation.\n",
    "\n",
    "<img src=\"attachment:056512c9-2ce6-438b-8d98-aeea63ab2da5.png\" alt=\"Alt Text\" width=\"500\">\n",
    "\n",
    "- we can understand it in a linear model:\n",
    "  $ y = wx + b $\n",
    "  here, w is weight which indicates variance and b is constant indicating bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be071fc9-833c-4206-bc26-285221039b89",
   "metadata": {},
   "source": [
    "# Feature Extraction and Selection\n",
    "\n",
    "- when model does not has enough features -> underfits\n",
    "- when model has too many features -> overfits\n",
    "\n",
    "## what is feature?\n",
    "\n",
    "Features are the **inputs we provide** to our model to make predictions.\n",
    "\n",
    "**Simplest Case**: In the simplest scenarios, features are straightforward. For instance, if we want to predict someone's salary based on their years of experience, then years of experience is our sole feature.\n",
    "\n",
    "**Complex Data**: As data becomes more intricate, feature extraction becomes crucial.\n",
    "\n",
    "For example, in building a spam filter, we need to extract features from raw email data, such as the \n",
    "(a) presence of specific words like \"Viagra\" or sender's domain.\n",
    "\n",
    "## Feature Types\n",
    "\n",
    "Features typically fall into three categories:\n",
    "1. binary (yes or no),\n",
    "2. numeric (quantitative), or\n",
    "3. categorical (discrete choices)\n",
    "  \n",
    "For example in email spam filter our features can be:\n",
    "1. if text contains term \"Viagra\" - binary(yes/no)\n",
    "2. how many times word d is mentioned - numeric\n",
    "3. what is domain of sender - from a set of discrete options\n",
    "\n",
    "## Model types based on features\n",
    "Different types of models are suited to different feature types.\n",
    "\n",
    "1. **Naive Bayes Classifier**: Suited for binary features.\n",
    "\n",
    "2. **Regression Models**: Require numeric features, including dummy variables.\n",
    "\n",
    "3. **Decision Trees**: Can handle both numeric and categorical data.\n",
    "\n",
    "## Feature Reduction\n",
    "\n",
    "- Sometimes, rather than creating new features, it's beneficial to reduce the number of features.\n",
    "\n",
    "- Techniques like <u> dimensionality reduction </u>  or <u>regularization</u>  help achieve this by distilling down hundreds of features to a few significant dimensions or penalizing models for using excessive features.\n",
    "  \n",
    "## Feature Selection Process\n",
    "\n",
    "- Choosing features involves a mix of experience and domain expertise. \n",
    "\n",
    "- For example, in spam filtering, knowing which words are indicative of spam and which are not, comes from experience.\n",
    "\n",
    "- Experimentation is often required to determine the most effective features, adding to the excitement of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0c1c6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
