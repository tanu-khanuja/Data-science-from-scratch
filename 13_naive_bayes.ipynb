{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fef6e0-7f70-4be9-b9dc-828f1fd40deb",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Simple yet powerful ML algorithm\n",
    "2. Used for <u>Classification tasks based on features</u>\n",
    "\n",
    "## Naive Assumption\n",
    "1. Assumes features are independent of each other, therefore called <u>Naive</u>\n",
    "\n",
    "## Algorithm Workflow\n",
    "1. Calculates probability of each class given the input features\n",
    "2. Selects class with highest probability as predicted class\n",
    "3. Uses Bayes' theorem to update probability based on observed features\n",
    "\n",
    "## Key Features\n",
    "1. Simple and computationally efficient\n",
    "2. Particularly effective for text classification tasks like <u>spam detection and document categorization</u>.\n",
    "3. Performs well in practice, even with the \"naive\" independence assumption.\n",
    "\n",
    "## Applications\n",
    "\n",
    "1. Widely used in various applications due to its speed, simplicity, and effectiveness\n",
    "2. Especially <u>suitable for high-dimensional data with many features</u>\n",
    "\n",
    "## A Very Basic Example\n",
    "\n",
    "Suppose we have a dataset of emails, where each email is represented by two features: \n",
    "1. the presence of the word \"lottery\" (0 or 1) \n",
    "2. the presence of the word \"free\" (0 or 1)\n",
    "\n",
    "The target variable is whether the email is spam (1) or not spam (0).\n",
    "\n",
    "| Email ID | Lottery | Free | Spam |\n",
    "|----------|---------|------|------|\n",
    "| 1        | 1       | 0    | 1    |\n",
    "| 2        | 0       | 1    | 1    |\n",
    "| 3        | 0       | 0    | 0    |\n",
    "| 4        | 1       | 1    | 1    |\n",
    "| 5        | 0       | 0    | 0    |\n",
    "\n",
    "Now, let's say we receive a new email with the following features: \"lottery\" (1) and \"free\" (0).\n",
    "\n",
    "Using Naive Bayes, we want to predict whether this new email is spam or not spam.\n",
    "\n",
    "**The algorithm works as follows:**\n",
    "\n",
    "1. <u>Calculate Class Probabilities:</u>\n",
    "\n",
    "Compute the probability of spam and not spam emails in the dataset. In this case, P(Spam) = 3/5 and P(Not Spam) = 2/5.\n",
    "\n",
    "2. <u>Calculate Feature Probabilities:</u>\n",
    "\n",
    "For each feature (lottery and free), compute the conditional probabilities of the feature given the class (spam or not spam).  \n",
    "\n",
    "For example, P(Lottery=1 | Spam) = 2/3 (as 2 out of 3 spam emails contain \"lottery\").\n",
    "Similarly, P(Lottery=1 | Not Spam) = 1/2 and P(Free=0 | Not Spam) = 1.\n",
    "\n",
    "3. <u>Calculate Posterior Probabilities:</u>\n",
    "\n",
    "Use Bayes' theorem to calculate the posterior probabilities of each class given the features.  \n",
    "\n",
    "For the new email, calculate P(Spam | Lottery=1, Free=0) and P(Not Spam | Lottery=1, Free=0) using the conditional probabilities and class probabilities.\n",
    "\n",
    "4. <u>Predict the Class:</u>\n",
    "\n",
    "Choose the class with the highest posterior probability as the predicted class.\n",
    "\n",
    "In this case, if P(Spam | Lottery=1, Free=0) > P(Not Spam | Lottery=1, Free=0), then predict spam; otherwise, predict not spam.\n",
    "\n",
    "5. <u>Result:</u>\n",
    "\n",
    "Based on the computed probabilities, we predict whether the new email is spam or not spam.\n",
    "This is a basic example of how Naive Bayes works for classification tasks. It's simple yet effective, especially for text classification problems like spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d1853-de4f-4641-8c11-521f9f7d79c9",
   "metadata": {},
   "source": [
    "# 1. A Really Dumb Spam Filter\n",
    "\n",
    "- Consider 2 events:\n",
    "  1. S : \"the message is spam\"\n",
    "  2. B : \"the message contains word `bitcoin`\n",
    " \n",
    "- According to Bayes' theorem : Probability of a message is spam when message contains word bitcoin is - \n",
    "  $$ P(S|B) = \\frac{P(B|S).P(S)}{P(B|S).P(S) + P(B|¬S).P(¬S)}$$\n",
    "\n",
    "- This simply represents the proportion of bitcoin messages that are spam.\n",
    "\n",
    "- **Assumption**\n",
    "  1. We have a sample data of known 'spam' and 'not spam' - thus we can estimate \n",
    "     $ P(B|S)$ and $P(B|¬S)$\n",
    "  3. Any new message has equal probability of being 'spam' or 'not spam' - thus $P(S) = P(¬S) = 0.5$\n",
    "\n",
    "     So, from Bayes' theorem:\n",
    "   >  $$ P(S|B) = \\frac{P(B|S).P(S)}{P(B|S).P(S) + P(B|¬S).P(¬S)}$$\n",
    "   >  $$ P(S|B) = \\frac{P(B|S).P(S)}{P(S).(P(B|S) + P(B|¬S)}$$\n",
    "   >  $$ P(S|B) = \\frac{P(B|S)}{P(B|S) + P(B|¬S)}$$\n",
    "\n",
    "  **Example**\n",
    "  if 50% of spam messages have the word bitcoin (P(B|S) = 0.5) but only 1% of nonspam messages do (P(B|-S) = 0.01), then the probability that any given bitcoin-containing email is spam is:\n",
    "$$ P(S|B) = \\frac{0. 5}{0. 5 + 0. 01} = 0.98 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcdf325-192b-46f1-8505-eaa3dffba737",
   "metadata": {},
   "source": [
    "# 2. A More Sophisticated Spam Filter\n",
    "- Imagine we have a vocabulary of words $w_{1}, w_{2}, ... w_{n}$\n",
    "  > Event $X_{i}$ means ' the message contains $w_i$ '\n",
    "- Imagine **we have sample data** of spam and non-spam mesages from where we know:\n",
    "  > Probability that a spam message contains word $w_{i}$ is $P(X_{i}|S)$  \n",
    "  > and  \n",
    "  > Probability that a non-spam message contains word $w_{i}$ is $P(X_{i}|-S)$\n",
    "- Naive Bayes assumes that **presence of each word is independent of each other** such that (Extreme Assumption):\n",
    "  > $P(X_{1} = x_{1}, . . . ,X_{n} = x_{n}|S) = P(X_{1} = x_{1}|S) ×⋯× P(X_{n} = x_{n}|S)$\n",
    "  - This is an extreme assumption.\n",
    "  - Imagine we have 100 spam messages.\n",
    "  - Out of it, 50 spam messages contain spam word 'bitcoin' and 50 messages contain spam word 'rolex'.\n",
    "  - No spam message contains both words together thus probability of spam message to contain both words becomes should be 0.\n",
    "  - But as both words are considered independent occuring, it comes from formula:\n",
    "  > $P(X_{1} = 1, X_{2} = 1|S) = P(X_{1} = 1|S) × P(X_{2} = 1|S)$  \n",
    "  > $P(X_{1} = 1, X_{2} = 1|S) = 0.5 × 0.5 = 0.25$  \n",
    "  > Despite the unrealisticness of this assumption, this model often performs well and has historically been used in actual spam filters.\n",
    "\n",
    "- Using Bayes' theorem we can compute $P(S|X=x)$\n",
    "  > $$ P(S|X=x_{i}) = \\frac{P(x_{i}|S)}{P(x_{i}|S) + P(x_{i}|¬S)}$$\n",
    "  > To compute the probability of message to be spam if it has list of words $x_{1}, x_{2},..., x_{n}$ multiply individual probability estimates\n",
    "\n",
    "- **Problem of Underflow**: we want to avoid multiplying lots of probabilities together, to prevent a problem called underflow, in which computers don’t deal well with floating-point numbers that are too close to 0. Computer will multiply many very small numbers (e.g. 0.0001 x 0.00025 x ...) and eventually after many multiplications it will reduce to 0\n",
    "  > So, to avoid this we use - $log(ab) = log a + log b$ and $exp(log x) = x$   \n",
    "  > It is floating-point-friendlier \n",
    "\n",
    "- **Problem of 0 probability of any word**:\n",
    "  > **How do we calculate $P(x_{i}|S)$ and $P(x_{i}|-S)$?** -- From sample data -- simply as the fraction of spam messages containing the word wi.    \n",
    "  > **Problem in this?** - Imagine that in our training set the vocabulary word `data` only occurs in nonspam messages. Then we’d estimate P(`data`|S) = 0. The result is that our Naive Bayes classifier would always assign spam probability 0 to any message containing the word data, even a message like “data on free bitcoin and authentic rolex watches.”    \n",
    "  > **How to solve this?** -- we’ll choose a pseudocount—k—and estimate the probability of seeing the $i^{th}$ word in a spam message as:  \n",
    "$$ P (Xi|S) = \\frac{k + number\\_of\\_spams\\_containing\\_wi}{2k + number\\_of\\_spams} $$    \n",
    "  > We do similarly for $P(Xi|¬S)$. That is, when computing the spam probabilities for the $i^{th}$ word, we assume we also saw k additional nonspams containing the word and k additional nonspams not containing the word.  \n",
    "  > For example, if 'data' occurs in 0/98 spam messages, and if k is 1, we estimate P(data|S) as 1/100 = 0.01, **which allows our classifier to still assign some nonzero spam probability** to messages that contain the word 'data'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb696f-c0c8-4a8b-b22d-8f3d87c3b6d0",
   "metadata": {},
   "source": [
    "# 3.Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b9ef2-a681-49ce-83a8-819c37ea1355",
   "metadata": {},
   "source": [
    "## Without functions - Basic code\n",
    "> 1. We have messages in a named tuple format of (text: str, is_spam: bool)\n",
    "> 2. First we will tokenize our texts and keep it in a dict of {text: List[words]}\n",
    "> 3. Then we will check if the messgae is_spam - Count no. of spam messages, no. of ham messages, frequency of tokens in spam messages, frequency of tokens in ham messages\n",
    "> 4. Calculate probability of each token in spam and ham messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f197844d-9ccf-4f73-9dbb-3bdc9ec257e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message.text='Congratulations Tanu, you have won 0.5 BTC. Click here to claim your BTC now'\n",
      "message.is_spam=True\n",
      "message.text='Hey Tanu, you won a new gift! Click here to buy your Rolex now'\n",
      "message.is_spam=True\n",
      "message.text='Dear Tanu, this mail is a reply to your query'\n",
      "message.is_spam=False\n"
     ]
    }
   ],
   "source": [
    "# Our training data\n",
    "from typing import NamedTuple\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "    \n",
    "messages = [Message(text = 'Congratulations Tanu, you have won 0.5 BTC. Click here to claim your BTC now', is_spam = True),\n",
    "            Message(text = 'Hey Tanu, you won a new gift! Click here to buy your Rolex now', is_spam = True),\n",
    "            Message(text = 'Dear Tanu, this mail is a reply to your query', is_spam = False)]\n",
    "\n",
    "# Let's print the class\n",
    "for message in messages:\n",
    "    print(f\"{message.text=}\")\n",
    "    print(f\"{message.is_spam=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16a604c5-0469-48e2-a3ce-eef4cb0329a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_dict={Message(text='Congratulations Tanu, you have won 0.5 BTC. Click here to claim your BTC now', is_spam=True): {'tanu', 'won', 'click', 'claim', 'you', 'now', 'congratulations', 'here', 'have', 'to', 'your', 'btc', '0', '5'}, Message(text='Hey Tanu, you won a new gift! Click here to buy your Rolex now', is_spam=True): {'tanu', 'won', 'new', 'a', 'you', 'click', 'rolex', 'now', 'buy', 'here', 'your', 'to', 'gift', 'hey'}, Message(text='Dear Tanu, this mail is a reply to your query', is_spam=False): {'mail', 'tanu', 'reply', 'a', 'this', 'is', 'your', 'to', 'dear', 'query'}}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the training data\n",
    "\n",
    "import re\n",
    "\n",
    "tokens_dict = {}\n",
    "for message in messages:\n",
    "    text = message.text.lower()\n",
    "    words = re.findall(\"[a-z0-9']+\", text) # Returns a list of all non-overlapping matches in the string\n",
    "    words = set(words)\n",
    "    tokens_dict[message] = words\n",
    "\n",
    "# Let's print the dict of tokens\n",
    "print(f\"{tokens_dict=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a57f66cc-d518-42fb-ba3b-0acb0e2339f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'tanu': 2, 'won': 2, 'click': 2, 'claim': 1, 'you': 2, 'now': 2, 'congratulations': 1, 'here': 2, 'have': 1, 'to': 2, 'your': 2, 'btc': 1, '0': 1, '5': 1, 'new': 1, 'a': 1, 'rolex': 1, 'buy': 1, 'gift': 1, 'hey': 1})\n",
      "defaultdict(<class 'int'>, {'mail': 1, 'tanu': 1, 'reply': 1, 'a': 1, 'this': 1, 'is': 1, 'your': 1, 'to': 1, 'dear': 1, 'query': 1})\n"
     ]
    }
   ],
   "source": [
    "# Counts\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "spam_messages = 0\n",
    "ham_messages = 0\n",
    "spam_token_counts = defaultdict(int)\n",
    "ham_token_counts = defaultdict(int)\n",
    "\n",
    "for message in messages:\n",
    "    if message.is_spam:\n",
    "        spam_messages +=1 \n",
    "        for word in tokens_dict[message]:\n",
    "            spam_token_counts[word] +=1\n",
    "    else:\n",
    "        ham_messages +=1\n",
    "        for word in tokens_dict[message]:\n",
    "            ham_token_counts[word] +=1\n",
    "\n",
    "# Let's print the token counts in spam and ham\n",
    "\n",
    "print(spam_token_counts)\n",
    "print(ham_token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77630442-af85-40d8-bd9b-fec595126c67",
   "metadata": {},
   "source": [
    "## Naive Bayes' Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a79455-643c-47b9-944c-43fc51a7c769",
   "metadata": {},
   "source": [
    "1. Tokenize\n",
    "2. Define type of training data as Namedtuple\n",
    "3. Define classifier\n",
    "   > 1. init method \n",
    "   > 2. training by iteration\n",
    "   > 3. prob calculation from (k+spam_with_w_counts)/(2k+total_spam_counts)\n",
    "   > 4. predict probabilty of message being spam considering all tokens are independent to each other\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a08451c-4ae8-4ac5-97e1-f621ee0cc03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from typing import Set, Iterable, Tuple\n",
    "import re, math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def tokenize(text:str) -> Set[str]:\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)\n",
    "    return set(all_words)\n",
    "\n",
    "assert tokenize(\"Data science is science\") == {\"data\", \"science\", \"is\", \"science\"}\n",
    "\n",
    "\n",
    "# Define type for training data\n",
    "# For named tuple type we use class\n",
    "from typing import NamedTuple\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "\n",
    "# Classifer needs track of tokens, counts, labels etc\n",
    "# For this we create a class\n",
    "# Initialize all counters to 0\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k # smoothening factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts:  Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "   \n",
    "    # Method to train a bunch of messages\n",
    "    # Increment spam and ham message counts\n",
    "    # tokenize spam and ham message and keep count track in dicts\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "\n",
    "        # Increment spam and ham counts\n",
    "        for message in messages:\n",
    "            if message.is_spam:\n",
    "                self.spam_messages +=1\n",
    "            else:\n",
    "                self.ham_messages +=1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] +=1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] +=1\n",
    "\n",
    "    # To predict P(spam|token) using Bayes' theorem\n",
    "    # We need P(token|spam) and P(token|ham)\n",
    "    # Create private helper function for this\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        returns P(token|spam) and P(token|ham)\n",
    "        \"\"\"\n",
    "        spam = self.token_spam_counts[token] # number of 'tokens' in spam messages\n",
    "        ham = self.token_ham_counts[token]   # number of 'tokens' in ham messages\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) /(self.ham_messages + 2* self.k)\n",
    "        return p_token_spam, p_token_ham\n",
    "        \n",
    "    # Finally predict using log and exp method instead of multiplication of small p's\n",
    "   \n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_p_if_spam = log_p_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in our vocab \n",
    "\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If 'token' appears in the message,\n",
    "            # Add log prob of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_p_if_spam += math.log(prob_if_spam)\n",
    "                log_p_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # Otherwise add the log probability of not seeing it\n",
    "            else:\n",
    "                log_p_if_spam  += math.log(1.0 - prob_if_spam)\n",
    "                log_p_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_p_if_spam)\n",
    "        prob_if_ham = math.exp(log_p_if_ham)\n",
    "        return prob_if_spam/(prob_if_spam+prob_if_ham)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1ec19-8df8-412e-88e3-d707098e7b7a",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85bce4e9-b1ab-41c4-8c66-ce60d4596cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model.k =0.5,\n",
      " model.tokens={'spam', 'ham', 'rules', 'hello'},\n",
      " model.token_spam_counts=defaultdict(<class 'int'>, {'spam': 1, 'rules': 1}),\n",
      " model.token_ham_counts=defaultdict(<class 'int'>, {'rules': 1, 'ham': 2, 'hello': 1}),\n",
      " model.spam_messages =1,\n",
      " model.ham_messages=2 \n"
     ]
    }
   ],
   "source": [
    "# Sample test data\n",
    "\n",
    "messages = [Message(\"spam rules\", is_spam=True),\n",
    "Message(\"ham rules\", is_spam=False),\n",
    "Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k = 0.5)\n",
    "model.train(messages)\n",
    "\n",
    "# Let's check what we got\n",
    "print(f\" {model.k =},\\n {model.tokens=},\\n {model.token_spam_counts=},\\n {model.token_ham_counts=},\\n {model.spam_messages =},\\n {model.ham_messages=} \")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bcade38-2bc4-4ea1-949b-02654f797848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8350515463917525"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make the prediction now\n",
    "\n",
    "text = \"hello spam\"\n",
    "model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cd9373c-3c3b-48b8-80b3-d5fab797afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate it by hand \n",
    "\n",
    "text = \"hello spam\"\n",
    "\n",
    "#probability of various words be present in spam and text\n",
    "prob_if_spam =  [\n",
    "(1 + 0.5) / (1 + 2 * 0.5), # \"spam\" (present in text)  \n",
    "1 - (0 + 0.5) / (1 + 2 * 0.5), # \"ham\" (not present)  \n",
    "1 - (1 + 0.5) / (1 + 2 * 0.5), # \"rules\" (not present)  \n",
    "(0 + 0.5) / (1 + 2 * 0.5) # \"hello\" (present)\n",
    "]\n",
    "\n",
    "#probability of various words be present in ham and text\n",
    "prob_if_ham =  [\n",
    "(0+0.5)/(2*0.5 + 2),  #'spam'(present)  \n",
    "(1- (2+0.5) / (2*0.5 + 2)),  #'ham'(not present)    \n",
    "(1- (1+0.5) / (2*0.5 + 2)), #'rules'(not present)   \n",
    "(1+0.5)/(2*0.5 + 2) #'hello'(present in text)   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c712b19-1629-43aa-9865-6ca05eee67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_if_spam = math.exp(sum(math.log(p) for p in prob_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in prob_if_ham))\n",
    "\n",
    "# Should be about 0.83\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3da0f-79cd-4af3-947d-1b245adf88e2",
   "metadata": {},
   "source": [
    "# 4. Using our model on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9c150fb-29c6-42dd-a6e2-973de2829e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unpack dataset\n",
    "\n",
    "import requests # To request file from url\n",
    "import tarfile # To open files in .tar format\n",
    "from io import BytesIO # So we can treat bytes as a file.\n",
    "\n",
    "\n",
    "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "FILES = [\"20021010_easy_ham.tar.bz2\",\n",
    "         \"20021010_hard_ham.tar.bz2\",\n",
    "         \"20021010_spam.tar.bz2\"]\n",
    "\n",
    "OUTPUT_DIR = 'spam_data' # This is where data will be downloaded\n",
    "\n",
    "# Unzip all three FILES\n",
    "for filename in FILES:\n",
    "    content = requests.get(f\"{BASE_URL}/{filename}\").content\n",
    "    fin = BytesIO(content)\n",
    "    with tarfile.open(fileobj = fin, mode = 'r:bz2') as tf:\n",
    "        tf.extractall(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f795b0-df81-4ad3-ae4c-88f69ff8101e",
   "metadata": {},
   "source": [
    "- After downloading the data you should have three folders: spam, easy_ham, and hard_ham.\n",
    "  \n",
    "- Each folder contains many emails, each contained in a single file.\n",
    "\n",
    "- To keep things really simple, we’ll just look at the subject lines of each email.\n",
    "\n",
    "- How do we identify the subject line? When we look through the files, they all seem to <u>start with “Subject:”</u>. So we’ll look for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ed9f82e-b081-469c-9cf0-06adfb0fe081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob # Helps in searching for files matching a specified pattern\n",
    "import re\n",
    "\n",
    "# Create a Message type class\n",
    "from typing import NamedTuple\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "# spam_data is directory where files are present\n",
    "# * is the wildcard character used to find path patterns\n",
    "path = 'spam_data/*/*'\n",
    "\n",
    "# Create empty list of Message type\n",
    "data: List[Message] = []\n",
    "\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename # Set 1 if spam and 0 if ham\n",
    "    with open(filename, errors = 'ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \") \n",
    "                data.append(Message(subject, is_spam))\n",
    "                break\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7146e0e-2611-44a0-8b43-e739242c83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data\n",
    "import random\n",
    "from scratch.machine_learning import split_data\n",
    "\n",
    "random.seed(0)\n",
    "train_messages, test_messages = split_data(data, 0.75)\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00bf5171-cdad-4c73-bec6-d7995b8fec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 668, (True, True): 85, (True, False): 54, (False, True): 18})\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "predictions = [(message, model.predict(message.text))\n",
    "              for message in test_messages]\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability >0.5)\n",
    "                           for message, spam_probability in predictions)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edf987-776a-42d7-8c5f-c5114dd475a6",
   "metadata": {},
   "source": [
    "- According to confusion matrix our model predicts:\n",
    "  > spam as spam = 85   \n",
    "  > spam as ham = 54  \n",
    "  > ham as ham = 668  \n",
    "  > ham as spam = 18\n",
    "\n",
    "  > Precision = 85/(85+18) = 82%  \n",
    "  > Recall = 85/(85+54) = 61%  \n",
    "\n",
    "- Not bad for such a simple model where we are just focusing on subject of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f5782f8-919e-41cc-9b74-1801939265b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest_words ['500', 'assistance', 'account', 'attn', 'sale', 'zzzz', 'systemworks', 'money', 'adv', 'rates']\n",
      "hammiest_words ['spambayes', '2', 'users', 'razor', 'zzzzteana', 'sadev', 'ouch', 'apt', 'bliss', 'selling']\n"
     ]
    }
   ],
   "source": [
    "# Let's see which words are most and least indicative of spam\n",
    "\n",
    "def p_spam_given_token(token:str, model: NaiveBayesClassifier) -> float:\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "\n",
    "    return prob_if_spam / (prob_if_ham + prob_if_spam)\n",
    "\n",
    "words = sorted(model.tokens, key = lambda t: p_spam_given_token(t, model))\n",
    "print(\"spammiest_words\", words[-10:])\n",
    "print(\"hammiest_words\", words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982eb071-fba9-4d82-ba64-7f559ef53768",
   "metadata": {},
   "source": [
    "That's it!\n",
    "\n",
    "To get the better performance what can we do?\n",
    "- Look at the message content, not just the subject line. You’ll have to be careful how you deal with the message headers.\n",
    "\n",
    "- Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional min_count threshold and ignore tokens that don’t appear at least that many times.\n",
    "\n",
    "- The tokenizer has no notion of similar words (e.g., cheap and cheapest). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. For example, a really simple stemmer function might be:  \n",
    "`def drop_final_s(word):`         \n",
    "    `return re.sub(\"s$\", \"\", word)`\n",
    "\n",
    "  Difficult - people use Porter stemmer - https://tartarus.org/martin/PorterStemmer/\n",
    "\n",
    "- Although our features are all of the form “message contains word wi,” there’s no reason why this has to be the case. In our implementation, we could add extra features like “message contains a number” by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d49b81-eaa4-4a6b-a894-dfaff7f7c1c6",
   "metadata": {},
   "source": [
    "# 5. Naive Bayes using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9694837d-b293-4119-b20f-b3c8027aeeca",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Import libraries\n",
    "2. Prepare data\n",
    "3. Split data\n",
    "4. Vectorize features\n",
    "   > Vectorization assigns digits from 0 to each word of text in alphabatical order (vectorizer.vocabulary_)  \n",
    "   > When we fit the text in vectorizer it creates array of row = texts, column = words, in terms of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e034b8df-f352-4f80-b68d-49e1b7d71972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (4, 1)\t1\n",
      "  (4, 6)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 7)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample training data\n",
    "texts = [\"good movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good one\"]\n",
    "labels = [1, 0, 0, 1, 1]\n",
    "\n",
    "#Vectorize the texts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "vocab = vectorizer.vocabulary_\n",
    "\n",
    "#Let's see what's vocab and vectorized matrix\n",
    "assert vocab=={'good': 1, 'movie': 4, 'not': 5, 'did': 0, 'like': 3, 'it': 2, 'one': 6}\n",
    "print(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c16c3d03-1b5e-4d6d-8683-3632ce9fe492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the vectorized data X into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.5, random_state=42)\n",
    "print(X_train)\n",
    "\n",
    "# Train data in Naive classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f0371da-ae94-467b-b15b-c8b29f98c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanukhanuja/anaconda3/envs/dsfs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/tanukhanuja/anaconda3/envs/dsfs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/tanukhanuja/anaconda3/envs/dsfs/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test,predictions)\n",
    "print(accuracy)\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d78ecb4-934d-4bd6-a3cc-7d24547c1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import glob # Helps in searching for files matching a specified pattern\n",
    "import re\n",
    "\n",
    "random.seed(0)\n",
    "# Create a Message type class\n",
    "from typing import NamedTuple\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool\n",
    "\n",
    "# spam_data is directory where files are present\n",
    "# * is the wildcard character used to find path patterns\n",
    "path = 'spam_data/*/*'\n",
    "\n",
    "# Create empty list of Message type\n",
    "data: List[Message] = []\n",
    "\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename # Set 1 if spam and 0 if ham\n",
    "    with open(filename, errors = 'ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \") \n",
    "                data.append(Message(subject, is_spam))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51e8b309-3a88-4c3e-9ebb-bb2831456cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#split data\n",
    "train_messages, test_messages = train_test_split(data, train_size=0.75, random_state = 42)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform([message.text for message in train_messages])\n",
    "X_test = vectorizer.transform([message.text for message in test_messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a12a29c8-e60d-41dc-a860-fdf7f2eb3a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212121212121213"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, [message.is_spam for message in train_messages])\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy = accuracy_score([message.is_spam for message in test_messages], predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9785c9b2-40d8-4025-9691-fcc85e2a9a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix=Counter({(False, False): 660, (True, True): 100, (True, False): 49, (False, True): 16})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "confusion_matrix = Counter(zip([message.is_spam for message in test_messages], predictions))\n",
    "print(f\"{confusion_matrix=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "919acd48-52c0-44fa-a649-49b706f709e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13157894736842105"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scratch.machine_learning import recall, precision\n",
    "fp,fn,tn,tp = [confusion_matrix[(i,j)] for i in range(2)\n",
    "               for j in range(2)]\n",
    "\n",
    "recall(tp, fp, fn, tn)\n",
    "precision(tp, fp, fn, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6e324f9-ff1c-4b76-8117-00d71d0b5a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Data-science-from-scratch/scratch/machine_learning.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794d42a-fcc6-4e29-8c64-cbb4cdfb3062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435479-adfd-4c80-a1b2-c8eb692101b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d6a16-99f9-4944-b596-509b4c97c195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
