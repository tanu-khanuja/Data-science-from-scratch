{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c73919e-8b98-4ff0-af6a-67f43956a4ae",
   "metadata": {},
   "source": [
    "- Here, we will look at different ways to use data to make recommendations\n",
    "- like in Twitter, Netflix, Youtube etc make recoomendations that you want to follow or watch.\n",
    "- Let's think of a problem of <u>recommending new interests to a user based on her currently specified interests.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c57b3c5-5c3b-487e-9fca-4706f9721b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use this dataset\n",
    "\n",
    "users_interests = [\n",
    "[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "[\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "[\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "[\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "[\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "[\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "[\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "[\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "[\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "[\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "[\"statistics\", \"R\", \"statsmodels\"],\n",
    "[\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "[\"pandas\", \"R\", \"Python\"],\n",
    "[\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "[\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33135c5f-b9d9-45e0-9802-8f3cd5e1b1a4",
   "metadata": {},
   "source": [
    "# 1. Recommending What's popular\n",
    "- When the user is new, and we just have a list of their existing interests\n",
    "- Then we can suggest them what is popular to expand their interests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9406c6bb-5551-47ba-b13a-bd34b7df547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_interests=Counter({'Python': 4, 'R': 4, 'Big Data': 3, 'HBase': 3, 'Java': 3, 'statistics': 3, 'regression': 3, 'probability': 3, 'Hadoop': 2, 'Cassandra': 2, 'MongoDB': 2, 'Postgres': 2, 'scikit-learn': 2, 'statsmodels': 2, 'pandas': 2, 'machine learning': 2, 'libsvm': 2, 'C++': 2, 'neural networks': 2, 'deep learning': 2, 'artificial intelligence': 2, 'Spark': 1, 'Storm': 1, 'NoSQL': 1, 'scipy': 1, 'numpy': 1, 'decision trees': 1, 'Haskell': 1, 'programming languages': 1, 'mathematics': 1, 'theory': 1, 'Mahout': 1, 'MapReduce': 1, 'databases': 1, 'MySQL': 1, 'support vector machines': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "popular_interests = Counter(interest for user_interests in users_interests\n",
    "                           for interest in user_interests)\n",
    "print(f\"{popular_interests=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3612841-b193-4d76-a3c3-a2f63cf2c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest most popular interests\n",
    "from typing import List, Tuple\n",
    "\n",
    "def most_popular_new_interests(user_interests: List[str],\n",
    "                               max_results: int = 5) -> List[Tuple[str, int]]:\n",
    "    suggestions = [(interest, frequency) \n",
    "                   for interest, frequency in popular_interests.most_common()\n",
    "                   if interest not in user_interests]\n",
    "    return suggestions[:max_results]              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59bd9fa-9d9a-469d-a6c7-748b62f78eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 4),\n",
       " ('R', 4),\n",
       " ('statistics', 3),\n",
       " ('regression', 3),\n",
       " ('probability', 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For user 1 with his user_interests [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"]\n",
    "# New most popular suggestions would we \n",
    "most_popular_new_interests([\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca580168-2c6e-471e-bb2c-027dd7bee6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R', 4), ('Big Data', 3), ('HBase', 3), ('Java', 3), ('statistics', 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For user 3 suggestions would be\n",
    "most_popular_new_interests([\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8805053-1e40-4a74-b596-c72bf05c5a58",
   "metadata": {},
   "source": [
    "# 2. User-based collaborative filtering\n",
    "\n",
    "- Here, we search for users with similar interests as our target user.\n",
    "- Then we use their interests to suggest them to target users.\n",
    "- We do this by finding similiarity of two users using *cosine similarity*\n",
    "\n",
    "**How do we do this?**.\n",
    "- 1. We create a Unique Interests List with all the interests of all the users. (unique_interests)\n",
    "- 2. Then we create a User Interest Vector based on above list. (1 at repective user's interests and rest 0)(user_interest_vector)\n",
    "- 3. Then we calculate similarity between each user pair using cosine similarity. (user_similarities)\n",
    "- 4. Then for each interest in lists of all similar users, we will add the its calculated similarity with target user\n",
    "- 5. Sort it and suggest the max score new interest to target user.\n",
    "\n",
    "**Understanding with Example**\n",
    "- Imagine you have a few users with their respective interests:\n",
    "\n",
    "> - User 0: [\"Python\", \"Data Science\"]\n",
    "> - User 1: [\"Machine Learning\", \"Data Science\"]\n",
    "> - User 2: [\"Python\", \"Artificial Intelligence\"]\n",
    "> - User 3: [\"Machine Learning\", \"Python\"]\n",
    "\n",
    "- Goal:\n",
    "We want to suggest new interests to User 0 based on the interests of users who are similar to User 0.\n",
    "\n",
    "- Steps:\n",
    "\n",
    "1. Identify Similar Users:  \n",
    "\n",
    "- Let's say, after calculating similarities using cosine similarity, we find:\n",
    "> - User 1 has a similarity score of 0.7 with User 0.\n",
    "> - User 2 has a similarity score of 0.8 with User 0.\n",
    "> - User 3 has a similarity score of 0.6 with User 0.\n",
    "\n",
    "2. Collect Interests of Similar Users:\n",
    "\n",
    "> - User 1's interests: [\"Machine Learning\", \"Data Science\"]\n",
    "> - User 2's interests: [\"Python\", \"Artificial Intelligence\"]\n",
    "> - User 3's interests: [\"Machine Learning\", \"Python\"]\n",
    "\n",
    "3. Aggregate Interest Scores:\n",
    "\n",
    "- For each interest from the similar users, add the similarity score of the user who has that interest.\n",
    "- \"Machine Learning\" is mentioned by User 1 and User 3: Score: 0.7 (from User 1) + 0.6 (from User 3) = 1.3\n",
    "- \"Artificial Intelligence\" is mentioned by User 2: Score: 0.8 (from User 2) = 0.8\n",
    "- \"Data Science\" and \"Python\" are already in User 0's interests, so we ignore them for new suggestions.\n",
    "\n",
    "4. Sort and Suggest New Interests:\n",
    "\n",
    "- Sort interests by score: [(\"Machine Learning\", 1.3), (\"Artificial Intelligence\", 0.8)]\n",
    "- Suggest the top interests not already in User 0's list:\n",
    "- Suggested Interests: [\"Machine Learning\", \"Artificial Intelligence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5b9ba1-74ec-4735-b93f-12ea673e5431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_interests=['Big Data', 'C++', 'Cassandra', 'HBase', 'Hadoop', 'Haskell', 'Java', 'Mahout', 'MapReduce', 'MongoDB', 'MySQL', 'NoSQL', 'Postgres', 'Python', 'R', 'Spark', 'Storm', 'artificial intelligence', 'databases', 'decision trees', 'deep learning', 'libsvm', 'machine learning', 'mathematics', 'neural networks', 'numpy', 'pandas', 'probability', 'programming languages', 'regression', 'scikit-learn', 'scipy', 'statistics', 'statsmodels', 'support vector machines', 'theory']\n"
     ]
    }
   ],
   "source": [
    "# Find a sorted set of unique interests of all users\n",
    "\n",
    "unique_interests = sorted(set(interest for user_interests in users_interests\n",
    "                          for interest in user_interests))\n",
    "print(f\"{unique_interests=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bc312a-dedb-480f-aca9-005a135b8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_1_interest_vector: [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# interest vector for each user\n",
    "\n",
    "Vector = List[int]\n",
    "\n",
    "def make_user_interest_vector(user_interests: List[str]) -> Vector:\n",
    "    \"\"\"\n",
    "    Given a list of interests, produce a vector whose ith element is 1\n",
    "    if unique_interests[i] is in the list, 0 otherwise\n",
    "    \"\"\"\n",
    "    return [1 if interest in user_interests else 0 for interest in unique_interests]\n",
    "\n",
    "user_1_interests = [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"]\n",
    "v = make_user_interest_vector(user_1_interests)\n",
    "\n",
    "print(f\"user_1_interest_vector: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40ebb53-f664-4f6a-8667-d704ca3a06f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# interest vector for all users\n",
    "\n",
    "user_interest_vectors = [make_user_interest_vector(user_interests) for user_interests in users_interests]\n",
    "print(user_interest_vectors[:5])  # First five user's vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "145557ee-28e9-46aa-ad29-0a8b6774e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_similarities[0]=[1.0, 0.3380617018914066, 0.0, 0.0, 0.0, 0.1543033499620919, 0.0, 0.0, 0.1889822365046136, 0.5669467095138409, 0.0, 0.0, 0.0, 0.1690308509457033, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise similarities\n",
    "\n",
    "from scratch.nlp import cosine_similarity\n",
    "\n",
    "user_similarities = ([[cosine_similarity(interest_vector_i, interest_vector_j) for interest_vector_i in user_interest_vectors]\n",
    "                      for interest_vector_j in user_interest_vectors])\n",
    "print(f\"{user_similarities[0]=}\")  # User 0's similarity with other users \n",
    "assert 0.15 < user_similarities[0][5] < 0.16   # Very few shared interests between user0 and user5\n",
    "assert 0.56 < user_similarities[0][9] < 0.57   # Many shared interests between user0 and user9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe8748f-5c00-4e50-b676-f3b6ced9b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, 0.5669467095138409), (1, 0.3380617018914066), (8, 0.1889822365046136), (13, 0.1690308509457033), (5, 0.1543033499620919)]\n"
     ]
    }
   ],
   "source": [
    "# Function to return a list of users with their similarity with one user\n",
    "\n",
    "from typing import Tuple\n",
    "def most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n",
    "    pairs = [(id, similarity) for id, similarity in enumerate(user_similarities[user_id]) if similarity > 0 and id != user_id]\n",
    "    return sorted(pairs, key=lambda pair: pair[-1], reverse=True)\n",
    "\n",
    "print(most_similar_users_to(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ec5f873-14ce-4dbc-99af-daca7031e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make suggestions \n",
    "# Just add similarity for each interests\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def user_based_suggestions(user_id: int, include_current_interests: bool = False):\n",
    "\n",
    "    suggestions: Dict[str, float] = defaultdict(float)\n",
    "    \n",
    "    for other_user_id, similarity in most_similar_users_to(user_id):\n",
    "        for interest in users_interests[other_user_id]:\n",
    "            suggestions[interest] +=similarity\n",
    "            \n",
    "    # sort suggestions based on scores\n",
    "    suggestions = sorted(suggestions.items(), key= lambda pair: pair[-1], reverse=True)\n",
    "\n",
    "    # Exclude already existing interest of user_id\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return [(suggestion,weight) for suggestion, weight in suggestions\n",
    "                if suggestion not in users_interests[user_id]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9ae340d-c9c9-4ae6-b96c-6a03c7742f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapReduce', 0.5669467095138409),\n",
       " ('MongoDB', 0.50709255283711),\n",
       " ('Postgres', 0.50709255283711),\n",
       " ('NoSQL', 0.3380617018914066),\n",
       " ('neural networks', 0.1889822365046136),\n",
       " ('deep learning', 0.1889822365046136),\n",
       " ('artificial intelligence', 0.1889822365046136),\n",
       " ('databases', 0.1690308509457033),\n",
       " ('MySQL', 0.1690308509457033),\n",
       " ('Python', 0.1543033499620919),\n",
       " ('R', 0.1543033499620919),\n",
       " ('C++', 0.1543033499620919),\n",
       " ('Haskell', 0.1543033499620919),\n",
       " ('programming languages', 0.1543033499620919)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_based_suggestions(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72f514-710f-4f78-afbe-e4b948a154f8",
   "metadata": {},
   "source": [
    "- The weights calculated (based on similarities) are used to rank and order the suggestions, but these weights are not inherently meaningful beyond that purpose.\n",
    "\n",
    "- When the number of interests or items is small, user-based collaborative filtering can effectively suggest new interests. However, as the number of interests grows, it becomes increasingly difficult to find similar users, and recommendations may become less relevant or useful.\n",
    "\n",
    "- In high-dimensional spaces (lots of interests), vectors (representing users' interests) are generally far apart and point in different directions. This makes it challenging to identify users with genuinely similar interest profiles.\n",
    "\n",
    "- On a platform like Amazon, where a user might have bought thousands of items over many years, it's hard to find another user with a similar purchase history. Even if you find the \"most similar\" user, their interests might still be very different, leading to poor recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e2212-73d9-4500-ad03-02c507d65f31",
   "metadata": {},
   "source": [
    "# 3. Item-based collaborative filtering\n",
    "\n",
    "- Unlike user-based collaborative filtering, here we will sort suggestions based on similarities of interests.\n",
    "\n",
    "  \n",
    "**How will we do it?**\n",
    "\n",
    "1. We had user_interest_vector, right? like this:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d911ae22-fd5a-4f9a-966a-fc94f59a8c86",
   "metadata": {},
   "source": [
    "[[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1b358-7f12-4b25-9fb7-727e4006b9a2",
   "metadata": {},
   "source": [
    "2. We will transpose this matrix to get interest-user matrix, like this:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5076105-1514-40e3-bfd3-ab37d2a2c294",
   "metadata": {},
   "source": [
    "Interest:          User 0 | User 1 | User 2 | User 3 | User 4    \n",
    "Big Data:           1     |   0    |   1    |   0    |   1  \n",
    "Hadoop:             1     |   0    |   0    |   0    |   1  \n",
    "Java:               1     |   0    |   1    |   0    |   0  \n",
    "NoSQL:              0     |   1    |   0    |   0    |   0  \n",
    "MongoDB:            0     |   1    |   0    |   0    |   0  \n",
    "Cassandra:          0     |   1    |   0    |   0    |   0  \n",
    "Python:             0     |   0    |   1    |   1    |   0  \n",
    "R:                  0     |   0    |   0    |   1    |   0  \n",
    "statistics:         0     |   0    |   0    |   1    |   0  \n",
    "Machine Learning:   0     |   0    |   0    |   0    |   1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d50178-ea63-410d-87d1-16deb6066bc0",
   "metadata": {},
   "source": [
    "3. Then, we will find similarities between each interests, e.g. find similarity between these two vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cb257-210b-4bd2-a351-266d4815d04c",
   "metadata": {},
   "source": [
    "\"Big Data\": [1, 0, 1, 0, 1]\n",
    "\"Hadoop\": [1, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f8232-9129-411e-a8e0-95691b2c450f",
   "metadata": {},
   "source": [
    "4. Then we will add scores of interests similar to target users interests,\n",
    "5. sort and suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a4da57a-bf05-4114-8cf4-66083d3e89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_interest_vectors[0]=[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{user_interest_vectors[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60131c5a-5d74-4960-97bb-799914e7488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interest_user_matrix[0]=[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create transpose matrix\n",
    "from scratch.linear_algebra import shape\n",
    "\n",
    "interest_user_matrix = [[user_interest_vector[j] for user_interest_vector in user_interest_vectors]\n",
    "                        for j, _ in enumerate(unique_interests)]\n",
    "\n",
    "assert shape(interest_user_matrix) == (36,15)        # 36 interests and 15 users                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "assert shape(user_interest_vectors) == (15,36) \n",
    "print(f\"{interest_user_matrix[0]=}\")                 # See, user 0, 8, 9 show interest in first interest \"Big Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac6b58d1-02f1-46f0-80aa-a2b942d3525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.4082482904638631, 0.3333333333333333, 0.8164965809277261, 0.0, 0.6666666666666666, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.5773502691896258, 0.4082482904638631, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity between each row i.e. each interest\n",
    "# i.e. similarity of 36 interests with 36 interests\n",
    "interest_similarities =  [[cosine_similarity(interest_vector_i, interest_vector_j)\n",
    "                         for interest_vector_i in interest_user_matrix]\n",
    "                         for interest_vector_j in interest_user_matrix]\n",
    "\n",
    "assert shape(interest_similarities) == (36,36)\n",
    "print(interest_similarities[0]) # similarity between \"Big Data\" and all other interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6fb0b29-6282-4e7c-803a-6b94f9527028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_interests_to(interest_id: int):\n",
    "    similarities = interest_similarities[interest_id]\n",
    "    pairs = [(unique_interests[other_interest_id], similarity)\n",
    "             for other_interest_id, similarity in enumerate(similarities)\n",
    "             if interest_id != other_interest_id and similarity > 0]\n",
    "    return sorted(pairs, key= lambda pairs: pairs[-1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef461e2f-20fb-4beb-811d-01ec35548af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 0.8164965809277261),\n",
       " ('Java', 0.6666666666666666),\n",
       " ('MapReduce', 0.5773502691896258),\n",
       " ('Spark', 0.5773502691896258),\n",
       " ('Storm', 0.5773502691896258),\n",
       " ('Cassandra', 0.4082482904638631),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('HBase', 0.3333333333333333)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_interests_to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17bab493-5860-470f-b8ce-006e14904a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum up similarities of interests similar to target user\n",
    "\n",
    "# interests similar to target user's interest are:\n",
    "# print(f\" most_similar_interests = {most_similar_interests_to(0)}\")\n",
    "\n",
    "def item_based_suggestions(user_id: int,\n",
    "                           include_current_interests: bool = False):\n",
    "    # Add similar interests\n",
    "    suggestions = defaultdict(float)\n",
    "    user_interest_vector = user_interest_vectors[user_id]\n",
    "    # print(f\"{user_interest_vector=}\")\n",
    "\n",
    "    for interest_id, is_interested in enumerate(user_interest_vector):\n",
    "        if is_interested == 1:\n",
    "            # print(f\"{interest_id=}\")\n",
    "            similar_interests = most_similar_interests_to(interest_id)\n",
    "            # print(f\"{similar_interests=}\")\n",
    "            for interest, similarity in similar_interests:\n",
    "                suggestions[interest] += similarity\n",
    "        # print(suggestions)\n",
    "    \n",
    "    # Sort them by weight\n",
    "    suggestions = sorted(suggestions.items(),\n",
    "                         key = lambda pair: pair[-1],\n",
    "                         reverse= True)\n",
    "    # print(suggestions)\n",
    "\n",
    "    if include_current_interests:\n",
    "        return suggestions\n",
    "    else:\n",
    "        return[(suggestion, weight) \n",
    "               for suggestion, weight in suggestions\n",
    "               if suggestions not in users_interests[user_id]]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73f3bd96-a8df-46e7-a4b3-b2c7b2d95a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 4.146264369941973),\n",
       " ('Storm', 4.146264369941973),\n",
       " ('Hadoop', 3.9554550146924106),\n",
       " ('Cassandra', 3.547206724228547),\n",
       " ('Java', 3.3794454097708404),\n",
       " ('Big Data', 3.3794454097708404),\n",
       " ('HBase', 3.0461120764375074),\n",
       " ('MapReduce', 1.861807319565799),\n",
       " ('MongoDB', 1.3164965809277263),\n",
       " ('Postgres', 1.3164965809277263),\n",
       " ('NoSQL', 1.2844570503761732),\n",
       " ('MySQL', 0.5773502691896258),\n",
       " ('databases', 0.5773502691896258),\n",
       " ('Haskell', 0.5773502691896258),\n",
       " ('programming languages', 0.5773502691896258),\n",
       " ('artificial intelligence', 0.4082482904638631),\n",
       " ('deep learning', 0.4082482904638631),\n",
       " ('neural networks', 0.4082482904638631),\n",
       " ('C++', 0.4082482904638631),\n",
       " ('Python', 0.2886751345948129),\n",
       " ('R', 0.2886751345948129)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_based_suggestions(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5778026-084c-475a-8834-b067c0288cdc",
   "metadata": {},
   "source": [
    "# 4. Matrix Factorization for Recommendation\n",
    "\n",
    "- Sometimes, instead of just 1/0 interests matrix we have numeric ratings.\n",
    "- Example: Amazon review rating varies 1-5 stars.\n",
    "- In this section we’ll assume -- we have such ratings data and try to learn a model that can predict the rating for a given user and item.  \n",
    "\n",
    "\n",
    "1. User preference and matrix representation\n",
    "   \n",
    "   - Users' preferences can be represented as a [num_users, num_items] matrix.\n",
    "   - In scenarios like Amazon reviews, ratings (e.g., 1 to 5 stars) can be represented as numbers in the same matrix format.\n",
    "\n",
    "2. Predictive Modeling\n",
    "\n",
    "- Goal: Develop a model to predict ratings for a given user-item pair.\n",
    "- Latent Types: Assume each user and item has a latent \"type\" represented by a vector of numbers.\n",
    "- User Types: Represented as a [num_users, dim] matrix.\n",
    "- Item Types: Represented as a [dim, num_items] matrix.\n",
    "\n",
    "3. Matrix Factorization\n",
    "- Factoring Process: Decompose the preference matrix into the product of a user matrix and an item matrix.\n",
    "- The product of these matrices approximates the original user-item rating matrix.\n",
    "\n",
    "4. Dataset used\n",
    "- MovieLens 100k Dataset: Contains user ratings (0 to 5) for many movies.\n",
    "- Users rate only a subset of movies.\n",
    "- The system aims to predict ratings for movies a user hasn't rated.\n",
    "- Data download link: <link>http://files.grouplens.org/datasets/movielens/ml-100k.zip.<link>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064fd575-a62d-43dc-9c67-384913ddcc84",
   "metadata": {},
   "source": [
    "**Let's code matrix factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d48322-9e3b-4f8b-b61f-5ac2e9b43918",
   "metadata": {},
   "source": [
    "1. We will only use two files\n",
    "> - 1. u.item is pipe delimited file with many columns. But we require only first two columns (Movie id | Movie title)\n",
    "> - 2. u.data is tab delimited file with 4 columns. We need only first three columns (User id  Movie id  Rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a9bd4a-bd21-4d84-9310-c4d2387946b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables holding path of file\n",
    "\n",
    "MOVIES = \"ml-100k/u.item\"\n",
    "RATINGS = \"ml-100k/u.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fc951-bae9-4b90-a6af-2a91b6d371a3",
   "metadata": {},
   "source": [
    "2. Create a class of named tuple for easy handling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4b73da1-bf18-4908-acf0-acaa97cefd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Rating(NamedTuple):\n",
    "    user_id: str\n",
    "    movies_id: str\n",
    "    rating: float\n",
    "\n",
    "# The movie ID and user IDs are actually integers, but they’re not consecutive, which\n",
    "# means if we worked with them as integers we’d end up with a lot of wasted dimensions\n",
    "# (unless we renumbered everything). So to keep it simpler we’ll just treat them as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b60b1a-4305-4140-a0b8-d899940b7043",
   "metadata": {},
   "source": [
    "3. Read files and explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87b56ff6-7112-4d35-be74-6303ca469f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 'Toy Story (1995)'), ('2', 'GoldenEye (1995)'), ('3', 'Four Rooms (1995)'), ('4', 'Get Shorty (1995)'), ('5', 'Copycat (1995)')]\n"
     ]
    }
   ],
   "source": [
    "# We specify this encoding to avoid a UnicodeDecodeError.\n",
    "# See: https://stackoverflow.com/a/53136168/1076346.\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(MOVIES, encoding = \"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter = \"|\")\n",
    "    movies = {movie_id: title for movie_id, title, *_ in reader}   # 1st column is id, 2nd column in movie title\n",
    "print(list(movies.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef9a5d17-ca91-463d-9ae2-1082e8605178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rating(user_id='196', movies_id='242', rating=3.0), Rating(user_id='186', movies_id='302', rating=3.0), Rating(user_id='22', movies_id='377', rating=1.0), Rating(user_id='244', movies_id='51', rating=2.0), Rating(user_id='166', movies_id='346', rating=1.0)]\n"
     ]
    }
   ],
   "source": [
    "# List of [Rating]\n",
    "\n",
    "with open(RATINGS, encoding=\"iso-8859-1\") as f:\n",
    "    reader = csv.reader(f, delimiter = \"\\t\")\n",
    "    ratings = [Rating(user_id, movies_id, float(rating)) for user_id, movies_id, rating, _ in reader]\n",
    "\n",
    "print(ratings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5456c63d-9f3f-4b5b-9a8f-0cfc91f08cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(movies) == 1682\n",
    "assert len(list({rating.user_id for rating in ratings})) == 943  # becuase ratings is a dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a3a64-1262-40af-8ca9-b5bd21f58f54",
   "metadata": {},
   "source": [
    "3. Let's play with the data -- find average ratings of 'Star Wars' movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ad44240-8bf6-4042-94a9-1e2a3edd9097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'50': [], '181': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "# Create a dictionary of movie id and an empty list for each id of star wars movies\n",
    "\n",
    "star_wars_ratings: Dict[int,List] = {\n",
    "    movie_id: []\n",
    "    for movie_id, title in movies.items()\n",
    "    if re.search(\"Star Wars|Enpire Strikes Back|Jedi\", title)\n",
    "}\n",
    "star_wars_ratings  # star wars movie id : empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "def45312-891a-493f-99f8-db9a84d99d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over ratings, accumulating Star wars ones\n",
    "for rating in ratings:\n",
    "    if rating.movies_id in star_wars_ratings:\n",
    "        star_wars_ratings[rating.movies_id].append(int(rating.rating))  # movie_id : List of ratings by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "496c5f88-fdcf-4b98-854c-229c4212e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average rating of each movie\n",
    "avg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id) \n",
    "                for movie_id, title_ratings in star_wars_ratings.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7c542f4-ead5-477f-b286-40fa5a8dfd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.358491 Star Wars (1977)\n",
      "4.007890 Return of the Jedi (1983)\n"
     ]
    }
   ],
   "source": [
    "# print them in order\n",
    "for avg_rating, movie_id in sorted(avg_ratings, reverse= True):\n",
    "    print(f\"{avg_rating:2f} {movies[movie_id]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565986e-b090-4966-be52-b65bfa1489fc",
   "metadata": {},
   "source": [
    "4. Let's create a model to predict these ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f176bdc8-ef26-4b76-b559-1790f865d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ratings data for training/testing\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(ratings)\n",
    "\n",
    "split1 = int(len(ratings) * 0.7)\n",
    "split2 = int(len(ratings) * 0.85)\n",
    "\n",
    "train = ratings[:split1]  # 70% of data\n",
    "validation = ratings[split1:split2]  # 15% of data\n",
    "test = ratings[split2:] # 15% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f2002-19f2-4a27-b6c0-f27c6aee2219",
   "metadata": {},
   "source": [
    "4(a). Create a baseline model\n",
    "\n",
    "- We will calculate avereage of ALL ratings in the table.\n",
    "- It will provide a simple benchmark to compare against our complex model.\n",
    "- If the complex trained model does not outperform this baseline model, its not worth of extra effort.(it may be overfitted/poor features selection/impelementation error)\n",
    "- It allows us to measure error and improve our model.\n",
    "\n",
    "Overall, it helps us in ensuring that improvement in our mdoel's prediction is due to its capabilities. Not Random or by chance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7abd47e-cf54-4dc4-ad41-3e1556021bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2609526646939684\n"
     ]
    }
   ],
   "source": [
    "# Create a baseline model\n",
    "\n",
    "avg_rating = sum(int(rating.rating) for rating in train) / len(train)\n",
    "\n",
    "baseline_error = sum([(rating.rating - avg_rating) ** 2\n",
    "                     for rating in test]) / len(test)            # MSE error\n",
    "print(baseline_error)  # We hope to do better than MSE 1.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b91e6-8691-433b-bbf9-92febd7fea23",
   "metadata": {},
   "source": [
    "4(b). Embeddings\n",
    "\n",
    "1. We are making embeddings for users and movies. Embeddings are nothing but vectors representing these numbers (users/movies)\n",
    "2. Matrix product of these embeddings will give us a vector which will be converged to user ratings. We need to train our emebddings in such way that their product will approximate their ratings.\n",
    "3. We store embeddings in dict {ID:Vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9119c40c-45b4-4de5-a5bf-471be3b8ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.deep_learning import random_tensor\n",
    "\n",
    "EMBEDDING_DIM = 2 # 2 element vectors\n",
    "\n",
    "# Find unique ids\n",
    "\n",
    "user_ids = {rating.user_id for rating in ratings}   # set of user ids\n",
    "movie_ids = {rating.movies_id for rating in ratings}  # set of movie ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a1af994-80ee-4ae3-bebd-1fffedd76664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random vector per id\n",
    "\n",
    "user_vectors = {user_id: random_tensor(EMBEDDING_DIM) \n",
    "                for user_id in user_ids}\n",
    "movie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n",
    "                 for movie_id in movie_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38f4ed64-0bba-44be-b905-38dce61bfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write training loop\n",
    "from typing import List\n",
    "import tqdm\n",
    "from scratch.linear_algebra import dot\n",
    "\n",
    "def loop(dataset: List[Rating], learning_rate: float = None) -> None:\n",
    "    with tqdm.tqdm(dataset) as t:  # Initializes dataset with progress bar\n",
    "        loss = 0 \n",
    "        avg_loss = 0\n",
    "        for i, rating in enumerate(t):  # i is index of each row of dataset and rating is tuple\n",
    "            movie_vector = movie_vectors[rating.movies_id]\n",
    "            user_vector = user_vectors[rating.user_id]\n",
    "            predicted = dot(user_vector, movie_vector)\n",
    "            error = predicted - float(rating.rating)\n",
    "            loss += error**2\n",
    "\n",
    "            if learning_rate is not None:  # if learning rate is given\n",
    "                user_gradient = [error * m_j for m_j in movie_vector] # d(error**2)/d(u)) = 2*error*m\n",
    "                movie_gradient = [error * u_j for u_j in user_vector]\n",
    "\n",
    "                # Take gradient steps\n",
    "                for j in range(EMBEDDING_DIM):\n",
    "                    user_vector[j] -= learning_rate * user_gradient[j]   # In place updatation of user_vectors\n",
    "                    movie_vector[j] -= learning_rate * movie_gradient[j] # In place updation of movie_vectors\n",
    "            avg_loss = loss/(i+1)\n",
    "            if i%1000 == 0:\n",
    "                t.set_description(f\"avg loss: {avg_loss=}\")   # Calculate avg loss for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72ec6de5-82d0-401b-a289-17daec3a5669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "avg loss: avg_loss=0.9314589262782678: 100%|██████████| 70000/70000 [00:00<00:00, 534661.39it/s]\n",
      "avg loss: avg_loss=0.9970691628623705: 100%|██████████| 15000/15000 [00:00<00:00, 1197209.57it/s]\n",
      "avg loss: avg_loss=0.9189001503117463: 100%|██████████| 70000/70000 [00:00<00:00, 636801.78it/s]\n",
      "avg loss: avg_loss=0.9810445040724082: 100%|██████████| 15000/15000 [00:00<00:00, 1157773.32it/s]\n",
      "avg loss: avg_loss=0.8996865255800943: 100%|██████████| 70000/70000 [00:00<00:00, 647063.72it/s]\n",
      "avg loss: avg_loss=0.9663587952482031: 100%|██████████| 15000/15000 [00:00<00:00, 1151184.95it/s]\n",
      "avg loss: avg_loss=0.8827232259038751: 100%|██████████| 70000/70000 [00:00<00:00, 640368.52it/s]\n",
      "avg loss: avg_loss=0.9535585670634156: 100%|██████████| 15000/15000 [00:00<00:00, 1194164.56it/s]\n",
      "avg loss: avg_loss=0.8678858520642012: 100%|██████████| 70000/70000 [00:00<00:00, 808464.83it/s]\n",
      "avg loss: avg_loss=0.942482809725308: 100%|██████████| 15000/15000 [00:00<00:00, 401884.14it/s]\n",
      "avg loss: avg_loss=0.8548664049140258: 100%|██████████| 70000/70000 [00:00<00:00, 812939.64it/s]\n",
      "avg loss: avg_loss=0.932921128320963: 100%|██████████| 15000/15000 [00:00<00:00, 1152260.22it/s]\n",
      "avg loss: avg_loss=0.8433905693915608: 100%|██████████| 70000/70000 [00:00<00:00, 606769.67it/s]\n",
      "avg loss: avg_loss=0.9246787499087195: 100%|██████████| 15000/15000 [00:00<00:00, 1047772.71it/s]\n",
      "avg loss: avg_loss=0.8332314058955861: 100%|██████████| 70000/70000 [00:00<00:00, 612829.88it/s]\n",
      "avg loss: avg_loss=0.9175828986303326: 100%|██████████| 15000/15000 [00:00<00:00, 1107982.32it/s]\n",
      "avg loss: avg_loss=0.8242024056460288: 100%|██████████| 70000/70000 [00:00<00:00, 467221.11it/s]\n",
      "avg loss: avg_loss=0.911480349975539: 100%|██████████| 15000/15000 [00:00<00:00, 1213980.90it/s]\n",
      "avg loss: avg_loss=0.8161492792156926: 100%|██████████| 70000/70000 [00:00<00:00, 588334.52it/s]\n",
      "avg loss: avg_loss=0.9062344878416551: 100%|██████████| 15000/15000 [00:00<00:00, 1151016.47it/s]\n",
      "avg loss: avg_loss=0.8089432373090333: 100%|██████████| 70000/70000 [00:00<00:00, 611086.26it/s]\n",
      "avg loss: avg_loss=0.9017233306936774: 100%|██████████| 15000/15000 [00:00<00:00, 1106988.07it/s]\n",
      "avg loss: avg_loss=0.8024758634929767: 100%|██████████| 70000/70000 [00:00<00:00, 468647.79it/s]\n",
      "avg loss: avg_loss=0.8978384592549185: 100%|██████████| 15000/15000 [00:00<00:00, 1091698.07it/s]\n",
      "avg loss: avg_loss=0.796655241432701: 100%|██████████| 70000/70000 [00:00<00:00, 615254.47it/s] \n",
      "avg loss: avg_loss=0.894484461713786: 100%|██████████| 15000/15000 [00:00<00:00, 1105801.21it/s]\n",
      "avg loss: avg_loss=0.7914030023066267: 100%|██████████| 70000/70000 [00:00<00:00, 598520.58it/s]\n",
      "avg loss: avg_loss=0.8915785402149876: 100%|██████████| 15000/15000 [00:00<00:00, 905688.54it/s]\n",
      "avg loss: avg_loss=0.7866520360507363: 100%|██████████| 70000/70000 [00:00<00:00, 726843.79it/s]\n",
      "avg loss: avg_loss=0.8890500400059538: 100%|██████████| 15000/15000 [00:00<00:00, 378369.72it/s]\n",
      "avg loss: avg_loss=0.7823446842350573: 100%|██████████| 70000/70000 [00:00<00:00, 557303.24it/s]\n",
      "avg loss: avg_loss=0.8868397737137331: 100%|██████████| 15000/15000 [00:00<00:00, 387026.00it/s]\n",
      "avg loss: avg_loss=0.778431289042832: 100%|██████████| 70000/70000 [00:00<00:00, 746985.81it/s]\n",
      "avg loss: avg_loss=0.8848990893834188: 100%|██████████| 15000/15000 [00:00<00:00, 385770.63it/s]\n",
      "avg loss: avg_loss=0.774869011516654: 100%|██████████| 70000/70000 [00:00<00:00, 615455.67it/s] \n",
      "avg loss: avg_loss=0.8831886823360662: 100%|██████████| 15000/15000 [00:00<00:00, 777808.05it/s]\n",
      "avg loss: avg_loss=0.7716208555747551: 100%|██████████| 70000/70000 [00:00<00:00, 778892.74it/s]\n",
      "avg loss: avg_loss=0.881677197392104: 100%|██████████| 15000/15000 [00:00<00:00, 211969.14it/s]\n",
      "avg loss: avg_loss=0.768654847312947: 100%|██████████| 70000/70000 [00:00<00:00, 646095.45it/s]\n",
      "avg loss: avg_loss=0.8803397151938998: 100%|██████████| 15000/15000 [00:00<00:00, 783240.30it/s]\n",
      "avg loss: avg_loss=0.8977786363305494: 100%|██████████| 15000/15000 [00:00<00:00, 1167765.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# x = int(0.7 * len(ratings))\n",
    "# train_data2 = ratings[:x]\n",
    "\n",
    "learning_rate = 0.05\n",
    "for epoch in range(20):\n",
    "    learning_rate *= 0.9  #let's reduce LR with each step\n",
    "    # print(epoch, learning_rate)\n",
    "    loop(train, learning_rate=learning_rate)\n",
    "    loop(validation)  \n",
    "# print(movie_vectors)\n",
    "loop(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc75ff2-c300-4ab6-84ed-2028dc5b9e38",
   "metadata": {},
   "source": [
    "- The training model gives avg_loss of 0.76 after 20 epochs\n",
    "- In validation data we got avg_loss of 0.89\n",
    "- In test data it is 0.911\n",
    "- Now, we will see PCA to inspect the learned vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
