{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d24b4af2-a12b-4f16-9f76-92faf9d895ec",
   "metadata": {},
   "source": [
    "- In Linear regression chapter we were fitting a line relating 'Number of friends a user has' with 'Number of daily minutes' user spents in website.\n",
    "\n",
    "- What if we know additional data :\n",
    "  1. how many hours each of a user works each day\n",
    "  2. whether they have a PhD\n",
    "\n",
    "- We can use this data to improve our model\n",
    "\n",
    "> We hypothesize a linear model with more independent variables:\n",
    "> $$ minutes = \\alpha + \\beta_{1} . friends + \\beta_{2} . work hours + \\beta_{3} . phd + \\epsilon $$\n",
    "\n",
    "- Obviously, whether a user has a PhD is not a number—but, as we mentioned in Chapter 11, we can introduce a dummy variable that equals 1 for users with PhDs and 0 for users without, after which it’s just as numeric as the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6fec07-ea40-49be-b061-7bcbd0b03a3e",
   "metadata": {},
   "source": [
    "# 1. The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd18a7-d46b-4c2b-a921-61bb905fbfeb",
   "metadata": {},
   "source": [
    "- Model for Multiple Regression:\n",
    "> $$ y_i = \\alpha + \\beta_1 x_{i1} + \\beta_1 x_{i2} + ... + \\beta_k x_{ik}+ \\epsilon_i $$\n",
    "> Vector of parameters: $$ \\beta = [\\alpha, \\beta_1, \\beta_2, ..., \\beta_k]$$\n",
    "> Vector of independent variables and a constant 1: $$x_i = [1, x_{i1}, x_{i2},..., x_{ik}]$$\n",
    "> Eventually x will be a matrix with each row: (length of dataset) and column: (number of independent variables+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97155f3c-fd5a-4d42-b52d-8130b4bcac75",
   "metadata": {},
   "source": [
    "- Our model is the dot product of $\\beta$ and $x_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4b35eb9-7c4d-4349-85ba-5c7d379a7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import dot, Vector\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\" \n",
    "    Assumes that the first elements of x is 1\n",
    "    \"\"\"\n",
    "    return dot(x, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb3246-a35b-4822-bfa5-306a37781673",
   "metadata": {},
   "source": [
    "- Vector $x_i$ in in our case is: `[constant_term, number_of_friends, work_hours_per_day, phd]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1af8ada1-9dec-406f-ad61-4d07ea25f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 49, 4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1e493-8e9b-4a3b-8caa-44b79cd43bc6",
   "metadata": {},
   "source": [
    "# 2. Assumptions of the LSM (least square method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15daff77-149d-4e6c-8e59-13a3649c7ea9",
   "metadata": {},
   "source": [
    "**What we know:**\n",
    "$x$ is a matrix of size: (row) x (column) = (length of dataset) x (number of independent variables + 1)\n",
    "\n",
    "**Our Assumptions in $x$ are:**\n",
    "1. Columns in x are <u>linearly independent</u> of each other\n",
    "   > - There’s no way to write any one as a weighted sum of some of the others.  \n",
    "   > **Why this assumption?**  \n",
    "   > If two variables are related to each other, we will never be able to find the true values of their coefficient.\n",
    "\n",
    "\n",
    "2. Columns of x are all uncorrelated with the errors ε.  \n",
    "> **Why this assumption?**  \n",
    "> Means the variables must be very much independent and there should not be any other un-considered factor(consider that as error) that might affect multiple variables.  \n",
    "> When we ignore or miss out imporatnt factors/variables or our data is affected by uncertainities, our predictions might be inaccurate.\n",
    "\n",
    "**For clarity read this:**\n",
    "\n",
    "Imagine you're trying to predict how well students will perform on a test based on two factors: how many hours they study and how many hours they sleep the night before the test.\n",
    "\n",
    "Now, let's say there's a hidden factor that affects both the number of hours students study and the number of hours they sleep—for example, how much stress they're under. Students who are more stressed might study more and sleep less.\n",
    "\n",
    "If you don't account for this hidden factor (stress) and its effect on both studying and sleeping, your predictions might be off. Even though you're using studying and sleeping as predictors, they're indirectly influenced by stress, which is not included in your model.\n",
    "\n",
    "This hidden factor (stress) is like the \"error\" mentioned by the author. It represents the parts of students' test performance that you haven't accounted for in your model.\n",
    "\n",
    "So, when the author says that the predictors (represented by x) should be independent of the errors (ϵ), they're emphasizing the importance of making sure that the factors you're using to make predictions (like studying and sleeping) aren't influenced by other factors that you haven't considered (like stress). If they are, your predictions might be biased or inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97621ae0-b387-41af-9087-11022445f778",
   "metadata": {},
   "source": [
    "# 3. Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4ec73-cfc0-4909-a205-924dcf3651c2",
   "metadata": {},
   "source": [
    "Steps (same as Linear regression model but with vectors):\n",
    "1. Find squared error i.e. loss function  \n",
    "2. Minimize loss function with given iterations for finding best fit beta vector - using gradient descent.  \n",
    "   Steps for GD:\n",
    "    > We will use minibatch gradient descent. \n",
    "    > 1. Set a random initial value to beta vector\n",
    "    > 2. Divide dataset into batches\n",
    "    > 3. Calculate gradient vector for each x_i, y_i and take its vector mean\n",
    "    > 4. Update beta using gradient_step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e35635a0-9dc0-4158-983f-900ae0d86358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Error\n",
    "from typing import List\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "\n",
    "# Let's check\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4] # so prediction = 4 + 8 + 12 = 24\n",
    "assert error(x, y, beta) == -6\n",
    "assert squared_error(x, y, beta) == 36\n",
    "\n",
    "# Compute gradient of Loss function for each variable in x vector\n",
    "\n",
    "def sqerror_gradient(x: Vector, y:  float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "assert sqerror_gradient(x, y, beta) == [-12, -24, -36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f70b6ef-615d-4566-a7a6-abcc5085d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Gradient descent to find optimal beta\n",
    "\n",
    "# Function will work with any dataset\n",
    "import random\n",
    "import tqdm\n",
    "from scratch.linear_algebra import vector_mean\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],                  # rows = variables, column = for each y\n",
    "                      ys: List[float],                   # y value vector of complete dataset\n",
    "                      learning_rate: float = 0.001,      # step_size\n",
    "                      num_steps: int = 1000,             # iterations\n",
    "                      batch_size: int = 1                # stochastic\n",
    "                     ) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with a random guess of beta\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    \n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    return guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dabaae04-2a81-449c-8a25-4434f49badd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5029.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Applying this to our data\n",
    "\n",
    "inputs: List[List[float]] = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "\n",
    "from scratch.statistics import daily_minutes_good\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "beta = least_squares_fit(inputs, daily_minutes_good, learning_rate=0.001, num_steps= 5000, batch_size=25)\n",
    "assert 30.50 < beta[0] < 30.70 # constant\n",
    "assert 0.96 < beta[1] < 1.00 # num friends\n",
    "assert -1.89 < beta[2] < -1.85 # work hours per day\n",
    "assert 0.91 < beta[3] < 0.94 # has PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0428c9-bb58-4b37-8b0a-138230af866d",
   "metadata": {},
   "source": [
    "# 4. Interpreting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c6d0d9-68d1-473f-85f6-7bf504a056e1",
   "metadata": {},
   "source": [
    "The above result on our data fits the model as:\n",
    "\n",
    "> $$ minutes = 30. 58 + 0. 972  friends − 1. 87  work\\_hours + 0. 923  phd $$\n",
    "\n",
    "- All else being equal estimates:\n",
    "  > 1. Everyone independent of any variable correspond to half an hour on website\n",
    "  > 2. Each additional friend corresponds to one minute extra on website\n",
    "  > 3. Each one hour of work corresponds to 2 minute less time on website\n",
    "  > 4. Having a phd indicates one extra minutes spent on website.\n",
    "\n",
    "- What model doesn't tell us?\n",
    "  > About Interaction among the variables\n",
    "  \n",
    "  > What if someone has many friends and works for many hours or vice versa, model doesn't capture that    \n",
    "          - We can capture that by adding another variable :${friends}*{work\\_hours}$  \n",
    "  \n",
    "  > It’s possible that the more friends you have, the more time you spend on the site up to a point, after which further friends cause you to spend less time on the site.  \n",
    "          - We can capture this by adding another variable : $(no. of friends)^2$  \n",
    " \n",
    "- What's the matter then?\n",
    "  > Once we start adding variables, we need to worry about whether their coefficients “matter.”  \n",
    "  > Beacause there are no limits to the numbers of products, logs, squares, and higher powers we could add."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b25bcc-5407-466a-9bd1-83fd84ff55a8",
   "metadata": {},
   "source": [
    "# 5. Goodness of fit\n",
    "## (a) R-squared\n",
    "\n",
    "  $$ R^2 =  1 - \\frac{\\sum{(y_p - y)^2}}{\\sum{(y - \\bar{y})^2}} = 1 - \\frac{sum\\_of\\_squared\\_errors}{total\\_sum\\_of\\_squares}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ee6db0e-0d4d-4001-82a6-bd0a9e1281ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.simple_linear_regression import total_sum_of_squares\n",
    "\n",
    "def multiple_r_squared(xs: List[Vector], ys: List[float], beta: Vector) -> float:\n",
    "    sum_of_squared_errors = sum([error(x, y, beta)**2 for x, y in zip(xs, ys)])\n",
    "    return 1 - sum_of_squared_errors/ total_sum_of_squares(ys)\n",
    "\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta) < 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d45bc-94af-489a-baf3-d95b76857fe5",
   "metadata": {},
   "source": [
    "### Interpretation of R-squared \n",
    "> - For linear regression where daily_minutes_good was related to only one variable num_friends, R-squared was 0.32  \n",
    "> - **Increasing the no. of variables will neccesarily increase the R-squared**  \n",
    "> - Simple regression model is just the special case of the multiple regression model where the coefficients on “work hours” and “PhD” both equal 0.  \n",
    "> - The optimal multiple regression model will necessarily have an error at least as small as that one.\n",
    "\n",
    "## (b) Standard Errors of the Coefficients\n",
    "> - In multiple regression we estimate it to measure how certain we are about our estimates of each $β_i$.  \n",
    "> - The regression as a whole may fit our data very well, but if some of the independent variables are correlated(or irrelevant), their coefficients might not mean much.\n",
    ">   \n",
    ">   **Assumptions** \n",
    "    > 1. errors($\\epsilon_i$) are independent normal RV\n",
    "    > 2. mean( $\\epsilon_i$) = 0\n",
    "    > 3. Standard_deviation( $\\epsilon_i$) = $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79c071-cf00-48fa-a227-e19860137371",
   "metadata": {},
   "source": [
    "# 6. Digression: The Bootstrap\n",
    "## What's Bootstrapping?\n",
    "- Bootstrapping is like **taking many random samples from your original data and seeing how consistent your estimates are across these samples**. It helps you understand if your findings are robust or if they might just be a result of chance in your specific sample.\n",
    "\n",
    "## How to do Bootstrapping?\n",
    "- It helps us to deal with uncertainity. Here's how it works:\n",
    "1. <u>Resampling</u>: Create multiple bootstrapped samples by randomly selecting observations from the original data <u>with replacement</u>.\n",
    "2. <u>Estimating Relationships</u>: Conduct regression analysis for each bootstrapped sample to estimate the relationship between variables.\n",
    "3. <u>Aggregating Results</u>: Calculate the average relationship between variables and **assess variability across bootstrapped samples** to quantify uncertainty.\n",
    "\n",
    "## Example\n",
    "> - We want to compute median of a n-point data sample\n",
    "> - We use Median function\n",
    "> - But how confident can we be about our estimate?\n",
    "> - case 1: If all the data points in the sample are very close to 100, then it seems likely that the actual median is close to 100.\n",
    "> - case 2: If approximately half the data points in the sample are close to 0 and the other half are close to 200, then we can’t be nearly as certain about the median.\n",
    "> - If we could repeatedly get new samples, we could compute the medians of many samples and look at the distribution of those medians. Often we can’t.  \r",
    "> - In that case we can bootstrap new datasets by choosing n data points with replacement from our data. And then we can compute the medians of those synthetic datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c18e320e-84c2-4756-8ebc-55bfcabfb985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Callable\n",
    "X = TypeVar('X') # Generic type of data\n",
    "Stat = TypeVar('Stat') # Generic type for \"statistics\"\n",
    "\n",
    "# Generate one sample\n",
    "def bootstrap_sample(data: List[X]) -> List[X]:\n",
    "    \"\"\"\n",
    "    randomly samples len(data) elements with replacement\n",
    "    \"\"\"\n",
    "    return [random.choice(data) for _ in data]\n",
    "\n",
    "\n",
    "# Apply bootstraping on num_samples\n",
    "# i.e. calculate stats_fn for all data_samples\n",
    "def bootstrap_statistics(data: List[X], \n",
    "                         stats_fn: Callable[[List[X]], Stat], num_smaples: int) -> List[Stat]: \n",
    "    \"\"\"\n",
    "    evaluates stats_fn on num_samples bootstrap samples from data\n",
    "    \"\"\"\n",
    "    return [stats_fn(bootstrap_sample(data)) for _ in range(num_smaples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72214bf8-85fa-4a15-893c-ece1f118256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply case 1 and case 2 and bootstrap the result\n",
    "\n",
    "# Generate data for two cases\n",
    "# Case 1: 101 points all very close to 100\n",
    "close_to_100 = [99.5 + random.random() for _ in range(100)]\n",
    "\n",
    "# Case 2: 101 points, 50 of them near 0, 50 of them near 200\n",
    "far_from_100 = [99.5 + random.random()] + [random.random() for _ in range(50)] + [200+random.random() for _ in range(50)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e6c21-c381-4d4d-98bd-c38b7561898b",
   "metadata": {},
   "source": [
    "- If you compute the medians of the two datasets, both will be very close to 100. However, if you look at their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ef8f254-5cfc-4d3a-bc55-5d8e46e3cb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_close=[100.11557108827196, 100.05938899335561, 100.1118300766562, 100.09304402255714, 100.06345913012771, 100.10318562796138, 100.14340336470055, 100.08153852510031, 100.1118300766562, 100.13472124701516, 100.05810502222813, 100.08153852510031, 100.08153852510031, 100.1242581685434, 100.08980118353116, 100.09925201880961, 100.04869930383559, 100.08338203945503, 100.07767301245525, 100.0887091238533, 100.08761706417543, 100.06751074062068, 100.08549955181523, 100.07158087739279, 100.04935407870948, 100.04998327496307, 100.08153852510031, 100.12146522272609, 100.09304402255714, 100.09195196287928, 100.08338203945503, 100.07951652680995, 100.1108869734438, 100.11277814245466, 100.09304402255714, 100.16419928992713, 100.07756390239805, 100.07158087739279, 100.09973624477225, 100.05938899335561, 100.07565101416489, 100.09304402255714, 100.13014734041147, 100.09973624477225, 100.1118300766562, 100.19463366588492, 100.11183503924225, 100.11277814245466, 100.08799093616436, 100.1118300766562, 100.0865916114931, 100.1108869734438, 100.09304402255714, 100.1118300766562, 100.1127831050407, 100.09649340574627, 100.08549955181523, 100.08338203945503, 100.15117646429539, 100.04044176595919, 100.08338203945503, 100.10798436650104, 100.10358691751345, 100.08163403917015, 100.10703630070259, 100.16240629511074, 100.08338203945503, 100.08549955181523, 100.11557108827196, 100.11557605085801, 100.10318562796138, 100.0887091238533, 100.11277317986861, 100.09304402255714, 100.08365603746051, 100.09649340574627, 100.1108869734438, 100.1108869734438, 100.11557108827196, 100.08980118353116, 100.13471628442912, 100.08761706417543, 100.04044176595919, 100.07158087739279, 100.08153852510031, 100.11836899667533, 100.07565101416489, 100.1127831050407, 100.13014734041147, 100.07158087739279, 100.11077731231836, 100.09649340574627, 100.08153852510031, 100.08980118353116, 100.10703630070259, 100.09628686158311, 100.17606856657363, 100.08980118353116, 100.107979403915, 100.05812546955647]\n"
     ]
    }
   ],
   "source": [
    "from scratch.statistics import median, standard_deviation\n",
    "median_close = bootstrap_statistics(close_to_100, median, 100)\n",
    "print(f\"{median_close=}\")\n",
    "# Mostly near 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38f87f11-4c6a-4f3b-9522-783cae92fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median_far=[200.19230954124467, 99.61713429320852, 200.0456964935684, 200.01243631882932, 0.9665489030431832, 200.0456964935684, 200.25093266482213, 200.15768657212232, 0.9184820619953314, 200.00152422185673, 200.17481948445143, 0.9665489030431832, 0.9665489030431832, 99.61713429320852, 0.8383265651934163, 0.8383265651934163, 0.9100160146990397, 99.61713429320852, 200.25068733928512, 0.9610312802396112, 99.61713429320852, 200.0467796859568, 200.24013040782634, 99.61713429320852, 0.9805166506472687, 0.9610312802396112, 99.61713429320852, 0.9665489030431832, 200.23941596018597, 0.9805166506472687, 200.0467796859568, 0.9369691586445807, 200.0456964935684, 200.01243631882932, 0.9665489030431832, 0.8383265651934163, 200.19230954124467, 0.9805166506472687, 0.9100160146990397, 200.00152422185673, 0.9610312802396112, 200.0456964935684, 0.9100160146990397, 0.8454245937016164, 200.0456964935684, 0.9610312802396112, 200.0456964935684, 200.17481948445143, 0.9665489030431832, 0.9805166506472687, 0.9369691586445807, 0.9882351487225011, 99.61713429320852, 200.23941596018597, 200.15768657212232, 200.01243631882932, 200.25922692344722, 0.9882351487225011, 200.25093266482213, 200.17481948445143, 200.15768657212232, 0.8454245937016164, 200.25068733928512, 0.8383265651934163, 0.9665489030431832, 200.0467796859568, 0.9665489030431832, 200.00152422185673, 0.9184820619953314, 200.25093266482213, 200.0456964935684, 0.7315983062253606, 200.19230954124467, 200.00152422185673, 0.9805166506472687, 200.0456964935684, 0.8383265651934163, 0.9100160146990397, 0.9610312802396112, 99.61713429320852, 200.00152422185673, 0.9100160146990397, 200.24013040782634, 200.24013040782634, 200.0467796859568, 0.9805166506472687, 200.17481948445143, 0.9882351487225011, 0.8383265651934163, 200.00152422185673, 200.00152422185673, 0.9665489030431832, 200.0456964935684, 0.8007465903541809, 200.0456964935684, 0.9369691586445807, 0.8454245937016164, 99.61713429320852, 0.9184820619953314, 99.61713429320852]\n"
     ]
    }
   ],
   "source": [
    "median_far = bootstrap_statistics(far_from_100, median, 100)\n",
    "print(f\"{median_far=}\")\n",
    "# Some near 200, 100 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61eba420-a53f-4823-8c7f-b6a9fee33a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_deviation(close_to_100)=0.26538375636532185, standard_deviation(far_from_100)=99.98866947301684\n"
     ]
    }
   ],
   "source": [
    "# Let's check the standard deviations of these medians\n",
    "from scratch.statistics import standard_deviation\n",
    "print(f\"{standard_deviation(close_to_100)=}, {standard_deviation(far_from_100)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffe066-832e-4ec1-96e9-5c55dcd17f9a",
   "metadata": {},
   "source": [
    "- The standard_deviation of the first set of medians is close to 0, while that of the second set of medians is close to 100\n",
    "\n",
    "- This extreme a case would be pretty easy to figure out by manually inspecting the data, but in general that won’t be true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a422c5c-becf-40d8-bb32-c3bec5740da6",
   "metadata": {},
   "source": [
    "# 7. SE($\\beta$) - Standard Errors of Regression Coefficients\n",
    "\n",
    "**What does it do?** \n",
    "- The Uncertainity/variability associated with each $\\beta_i$\n",
    "\n",
    "**How we find it?**\n",
    "- We check the values of $\\beta_i$ for various samples from same dataset (using bootstrapping).\n",
    "- If coefficient changes much - means uncertainity is high and vice-versa.\n",
    "\n",
    "**Note -**\n",
    "- When bootstrapping we need to keep y_i and its corresponding x_i's preserved together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf9c1772-a882-424f-9e06-6313c7ca71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4943.65it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5183.06it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5220.69it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5217.61it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5213.98it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5045.16it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5182.09it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5162.02it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5166.82it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5178.27it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5172.75it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5195.38it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5192.16it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5185.38it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5186.74it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5012.09it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5164.55it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5152.85it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5180.85it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5167.57it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5140.01it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5168.36it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5027.08it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5180.84it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5178.41it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5165.68it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5167.89it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5164.25it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5170.77it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5160.89it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5182.02it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5150.00it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5009.46it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5157.34it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5147.51it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5144.28it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5085.81it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5109.68it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5136.79it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5112.09it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5140.89it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5011.60it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5116.16it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5131.88it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5114.44it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5086.13it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5099.97it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5105.59it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5128.31it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5001.00it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5132.03it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5132.63it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5130.21it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5126.82it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5132.00it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5130.16it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5133.54it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5126.20it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5141.44it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4999.97it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5135.17it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5125.43it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5125.69it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5123.79it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5124.14it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5121.44it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5113.14it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4968.22it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5110.93it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5098.58it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5108.51it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5119.52it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5104.02it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5104.13it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5098.50it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4967.28it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5107.13it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5100.90it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5076.61it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5039.29it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4946.97it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5135.56it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5122.45it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5088.90it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5041.72it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5082.24it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4937.54it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5079.31it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5070.01it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5075.17it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4942.08it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5079.91it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5072.58it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5078.23it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4941.66it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5086.16it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5103.84it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5068.71it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5080.39it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:00<00:00, 5072.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# Create a sample first \n",
    "# Estimation beta for that sample using least_square_fit function\n",
    "\n",
    "def estimate_sample_beta(pairs: List[Tuple[Vector, float]]) -> float:\n",
    "    x_sample = [x for x, _ in pairs]\n",
    "    y_sample = [y for _, y in pairs]\n",
    "    beta = least_squares_fit(x_sample, y_sample, learning_rate=0.001, num_steps=5000, batch_size=25)\n",
    "    return beta\n",
    "\n",
    "# Bootstrap beta\n",
    "\n",
    "random.seed(0)\n",
    "bootstrap_betas = bootstrap_statistics(list(zip(inputs, daily_minutes_good)), estimate_sample_beta, num_smaples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91bc1660-c751-4144-b3ca-4c7a5859be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30.49402029547432, 1.0393791030498776, -1.9516851948558502, 0.7483721251697333], [30.149963287526045, 1.0005300432763113, -2.0650380122822543, 3.177179854834797], [29.202826897693722, 1.0017089956376213, -1.5294248424787367, 0.9528580285760854], [31.29481217471851, 0.959264729494101, -1.9120875473727545, 0.039471107599519425], [32.124144227949955, 0.8569794405277468, -1.9936770520754086, 1.0416943131373024], [31.8691994453096, 0.7748022870492418, -2.0087625702876446, -1.2407036547656678], [31.08119759650208, 0.998386254386918, -1.9833984114987815, 0.9567646217580389], [29.254530450577782, 0.9763387220017684, -1.7430339427043595, 1.9944240584590935], [31.649174199331632, 0.9389340937491032, -1.9733848473304205, -0.15249287969349437], [30.040109260720964, 1.0531247386421572, -1.7694878560354388, 1.302971911084249], [29.066927054721297, 1.2792640005590372, -1.937339904947856, 0.9183668519320846], [31.740476303331718, 0.9538879291586574, -2.0689725879612477, 1.4785830120835612], [29.46654084062671, 0.9837739845117637, -1.9915052407093472, 3.150029950640157], [30.97515705531374, 0.9420086669374396, -2.0367671746636065, 0.6323599067111714], [31.478778128163995, 0.8623617407485805, -1.8798782324632368, -0.11949170941208796], [33.87286992682308, 0.8824018752321863, -1.8978803929581156, -1.0333647107478692], [29.272206898314987, 1.0899411603739348, -1.8911943299601002, 3.162677841885805], [30.83577809561691, 1.0242186355671827, -1.9209251081222494, 1.3383795133620962], [28.211162672015906, 1.458352440392638, -1.70241171517105, 0.94520401518726], [29.93552336056075, 0.9470529669956465, -1.8491245571618218, 0.8573641103651921], [30.636052325886002, 0.9966176913889679, -1.8308401560119625, 0.13862673979220685], [30.855945311129382, 0.9925731301194982, -1.834813509355548, 1.9711641797749935], [29.77226728370608, 1.0493381798575807, -1.6999309651266667, 0.9221651877575128], [28.78470035821775, 0.9629668755117143, -1.7818333154132011, 1.905170320676074], [31.769457992268443, 0.9040180814550004, -1.867677593282121, -0.7957987643064021], [30.06836087625883, 0.9237365767889174, -1.7326788050658604, 1.9044381512517516], [29.248924522774857, 1.0251706036709467, -1.6396068581125103, 1.7875039505127974], [26.160390418551746, 1.3566609275406472, -1.880731098382104, 3.884946816272215], [31.9708823034869, 0.8717159490168253, -1.8037586194211703, -0.23788897755135452], [30.580903591168788, 0.9610711598856186, -1.8984859248085817, -0.0023187395782722853], [31.433330253362577, 0.8768141821390377, -1.7328584033279486, -0.10210988051437449], [30.984236860566945, 1.036149466142919, -2.2200016449095226, 1.0886749895563579], [29.28237674477942, 1.0858388836439414, -1.74280602847473, 1.4397328297413239], [30.65460047430859, 0.9454408039075628, -1.7320071301269266, -0.1485862182089119], [29.118139496835955, 0.893808801696634, -1.9153563192896768, 2.0598345811489462], [29.95488463195938, 0.9940567914003665, -1.7605085370056377, 1.6096257131696945], [31.000164842544855, 0.962368315956188, -1.9115208623969564, 0.7473190835230143], [30.825204744897878, 0.8912590208026672, -1.770469093607492, 0.7459655949536621], [29.366136812382027, 1.012558241060829, -1.6182773155952548, 1.017025703754098], [29.94295970143512, 1.0167217566773747, -1.5621167917565122, -0.1030904763985423], [29.962898858207343, 1.0652251821283687, -1.926924147663584, 2.0385736378519947], [30.525302041791818, 0.9658944102293198, -1.8870631894489638, 0.5367690208128764], [30.6785320563259, 1.0139828545599132, -1.7817299670979692, 1.6026393229652947], [30.09073747870904, 1.0047123547747132, -1.9560265455918162, 2.75259429615735], [30.937038893678913, 0.9670590611928079, -2.1124811600264293, 0.3258045605146989], [28.78930855900356, 1.1730115746597942, -1.7835138640623003, 3.2623158308236095], [31.14754912309552, 0.9326436111603991, -1.7707952504307622, -1.099359064604309], [31.813727613195653, 0.9683784085384612, -2.019078886892217, 0.7501206686148623], [30.223398353230184, 0.9373764744862103, -1.5323607166675373, -0.01469994075209993], [28.01777500444891, 1.091598878794665, -1.6190191022832499, 2.397154344588124], [29.342668886496593, 0.9815156932180105, -1.9184777914462317, 1.5482939749639446], [32.53937166649288, 1.060883971208886, -2.2704689582768722, 0.3681597653761555], [30.106198499206915, 0.9657134612613777, -1.7191529436530641, -0.6267619207221298], [29.993282359977137, 0.9757399392816419, -1.9767875486880904, 2.048669364846268], [30.571136409586924, 1.066488813531558, -1.6618835177744289, -0.19985556821698586], [30.9490097252882, 0.9597396222139453, -1.9214823753987709, 1.25885503487694], [31.887007554673076, 0.9506671496957437, -2.152653973374404, 1.6869486505999165], [29.081704350215187, 1.0495038787355986, -1.6920009023683746, 3.609080049949202], [31.479546830562978, 1.1296437640969312, -1.8930013630375897, 0.2328971438009536], [30.610973912805395, 1.0065894319911013, -1.836243246680104, 0.4499397217455247], [31.80927695488258, 0.9821469730488941, -2.007959621103926, -0.2411398745050231], [31.024210851804416, 0.9515774062029452, -1.9408222914617927, 0.6442854716394794], [28.908141235990033, 1.0556273838810308, -1.7935754991375803, 2.082266951237433], [30.025383087071763, 0.9490311032868943, -1.8905462953821093, 1.614968102502849], [31.344911606937217, 0.9596230552550087, -2.084944019182774, 1.0635864768954955], [30.88785665879763, 0.9739691303740718, -1.750496781109518, -2.0086684580110616], [30.524172097277887, 0.9468432200060536, -1.7489583214704674, -0.42947540813439916], [33.73887281461898, 0.8342998931764716, -2.0056583070815233, -1.0048943591784738], [29.04731144829789, 0.9737448743420717, -1.7622553843049413, 0.9744871197165679], [30.849086975523765, 1.1142041012783979, -2.055393538038613, 1.8606960468590918], [31.20227902410509, 1.0148203879553739, -1.831139817867853, -0.12803605188562736], [30.44951242588852, 0.9188875408835141, -1.6623667661150159, 0.41561209518705605], [30.93743647937649, 0.9178249912706591, -1.91788395405578, 0.8027340312172657], [33.07304817934109, 0.7669188362229076, -1.8621104803815107, -0.5344373694611129], [30.98035452936738, 0.9608047189289309, -1.8571138381579286, 1.2456516010877], [29.890049201061156, 0.9320508621300003, -1.815157140889288, 1.6197634219660293], [32.7497073906742, 0.8163410438179741, -1.6727937223778233, -1.627203273138944], [32.23550207094589, 0.9915112587422378, -2.201685593411146, 0.659721525694611], [30.238346353105722, 0.9812068545490507, -1.9183149068660714, 2.4252389104819785], [30.574120085079127, 0.9174840515163696, -1.791824539551342, 0.9221993996446399], [30.200588272490595, 0.9290781608340558, -1.5128386060160508, -0.27164281164191667], [30.568001921567824, 1.0423323558239714, -2.0539328282484295, 2.070512986336468], [32.24170594033224, 0.928943846288939, -1.9597146432475416, -0.3283270089441192], [32.867472630955, 1.0159010210188608, -2.0279568468137548, -0.5177147877542921], [29.215869116992934, 1.0071212080144287, -1.9567505776484149, 3.7248516336467894], [29.731620363957564, 1.0022351904608418, -1.6056750069107464, 0.38365805637548667], [32.67347841435598, 0.8824434637692973, -1.9909101579029314, 0.04871947146702888], [29.15775553756214, 1.0683351454601346, -1.7096121511993116, 3.2616854857255317], [30.488240564960755, 1.0353317712496077, -1.9149562223503453, 2.595089595245561], [31.498485256154574, 0.865173722648589, -1.9003285713857743, -0.4448014961070426], [28.568637146495437, 0.9377084816305519, -1.6697079214548882, 2.0378528186926], [30.8880890003392, 0.9480046573855904, -1.9409732963472783, -0.38053847722698325], [30.449302174408764, 1.121851483678596, -1.9621516796699556, 2.244341597832513], [30.266204204539516, 1.006867325680496, -2.1198992898486466, 0.5362851256019128], [29.33031812613639, 1.0424517245684064, -1.8849226826885934, 2.2650387817258566], [31.777389538816966, 0.8928310423632744, -1.9269578522157438, 0.048635890062917325], [28.291072745509133, 1.1873361941623277, -1.8546687169062575, 2.6390276558088757], [31.725525991297584, 0.8939775539447468, -1.843559060469148, -0.6224324630864045], [30.2731194689144, 0.8005769229528958, -1.6991234036996576, 0.9748341305369917], [31.75696506368952, 1.0790800487199688, -2.0880894078200054, 1.6420943383461737]]\n"
     ]
    }
   ],
   "source": [
    "print(bootstrap_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb7cba29-a9b5-4461-ac2d-8bda65a0b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2715078186272781, 0.10318410116073963, 0.15510591689663628, 1.2490975248051257]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Standard deviation of each coefficient\n",
    "\n",
    "bootstrap_standard_errors = [standard_deviation([beta[i] for beta in bootstrap_betas]) \n",
    "                             for i in range(4)]\n",
    "print(bootstrap_standard_errors)\n",
    "\n",
    "# [1.271, # constant term, actual error = 1.19\n",
    "# 0.103, # num_friends, actual error = 0.080\n",
    "# 0.155, # work_hours, actual error = 0.127\n",
    "# 1.249] # phd, actual error = 0.998"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d9e571-eeba-4c1f-a7f9-591522de1970",
   "metadata": {},
   "source": [
    "- The predicted error and actual error are not exactly same but can be estimated better <u>if we had larger dataset.</u>\n",
    "\n",
    "## Testing Hypothesis\n",
    "\n",
    "- We can use these to test hypotheses such as “does βi equal 0?”\n",
    "\n",
    "- Checking if <u>coefficients are equal to zero</u> is an essential part of statistical analysis, especially in regression modeling, for several reasons:\n",
    "\n",
    "1. **Relevance of Predictor Variables**:  \n",
    "   > - Not all predictor(independent) variables may have a significant impact on the outcome.\n",
    "\n",
    "2. **Model Simplicity**:\n",
    "   > - Including irrelevant predictor variables in a regression model can lead to <u>overfitting</u>, where the model fits too closely to the training data but performs poorly on new data.  \n",
    "   > - By identifying and removing variables with coefficients close to zero, we can <u>simplify the model without losing predictive accuracy</u>.  \n",
    "\n",
    "3. **Interpretation of Results**:\n",
    "   > - Knowing which predictor variables have non-zero coefficients helps us interpret the results of the regression analysis more accurately.  \n",
    "   > - Variables with significant coefficients indicate a stronger relationship with the outcome variable, while variables with coefficients close to zero may not contribute much to the model's predictive power.  \n",
    "\n",
    "4. **Economic and Practical Significance**:\n",
    "   > - In some cases, even if a coefficient is statistically significant (i.e., not equal to zero), it may not be economically or practically significant.  \n",
    "   > - For example, a coefficient may be statistically significant but too small to have a meaningful impact in real-world applications. By testing if coefficients are zero, we can assess both statistical and practical significance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274e3ba-d45d-43f3-8694-314532c606b9",
   "metadata": {},
   "source": [
    "# 8. Regularization\n",
    "\n",
    "## (a) Problem with Many Variables\n",
    "> 1. When linear regression models have many variables, there's a risk of overfitting.\n",
    "> 2. More nonzero coefficients you have, the harder it is to make sense of them.\n",
    "\n",
    "## (b) Regularization\n",
    "> Regularization is an approach in which we <u> add to the error term a penalty</u> that gets larger as beta gets larger. \n",
    "> We then minimize the combined error and penalty.  \n",
    "> The more importance we place on the penalty term, the more we discourage large coefficients.\n",
    "\n",
    "### 1. Ridge Regression\n",
    "> Adds a penalty proportional to the sum of square of the coefficients (except for the constant term), often referred to $L_2$ regularization.\n",
    "$$ ridge\\_penalty = \\alpha . \\sum{(\\beta_i)^2} = \\alpha . (\\bar\\beta[1:] \\cdot \\bar\\beta[1:])$$  \n",
    "> $\\alpha$ is a *hyperparameter* controlling how harsh the penalty is.\r",
    "> Sometimes it's called \"lambda\" but that already means something in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47cb62dd-324d-4ff8-a128-023ff4106c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def ridge_penalty(beta: List[float], alpha: float) -> float:\n",
    "    return alpha * dot(beta[1:], beta[1:])\n",
    "\n",
    "def squared_error_ridge(x: List[float],\n",
    "                        y: float,\n",
    "                        beta: List[float],\n",
    "                        alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    estimate error plus ridge penalty on beta\n",
    "    \"\"\"\n",
    "    return error(x, y, beta) ** 2 + ridge_penalty(beta, alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d581a037-253d-4f40-9977-bfc08b99b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then plug this into gradient descent in the usual way:\n",
    "\n",
    "from scratch.linear_algebra import add\n",
    "\n",
    "def ridge_penalty_gradient(beta: List[float], alpha: float) -> List[float]:\n",
    "    \"\"\"gradient of just the ridge penalty\"\"\"\n",
    "    return [0.] + [2 * alpha * beta_j for beta_j in beta[1:]]\n",
    "\n",
    "def sqerror_ridge_gradient(x: List[float],\n",
    "                           y: float,\n",
    "                           beta: List[float],\n",
    "                           alpha: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    the gradient corresponding to the ith squared error term\n",
    "    including the ridge penalty\n",
    "    \"\"\"\n",
    "    return add(sqerror_gradient(x, y, beta),ridge_penalty_gradient(beta, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a05f4e46-0e93-4384-b48e-d63e1c084044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we just need to modify the least_squares_fit function to use\n",
    "# the sqerror_ridge_gradient instead of sqerror_gradient.\n",
    "Vector = List[float]\n",
    "\n",
    "def least_squares_fit_ridge(xs: List[Vector],                  # rows = variables, column = for each y\n",
    "                      ys: List[float],  alpha,# y value vector of complete dataset\n",
    "                      learning_rate: float = 0.001,      # step_size\n",
    "                      num_steps: int = 1000,             # iterations\n",
    "                      batch_size: int = 1                # stochastic\n",
    "                     ) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with a random guess of beta\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "    \n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "            gradient = vector_mean([sqerror_ridge_gradient(x, y, guess, alpha)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "90279cc1-e35e-44a3-b377-eaeb4f2bf443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 3162.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# With alpha set to 0, there’s no penalty at all and we get the same results as before:\n",
    "import random\n",
    "random.seed(0)\n",
    "learning_rate = 0.001\n",
    "beta_0 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.0, # alpha \n",
    "                                 learning_rate, 5000, 25)\n",
    "# [30.51, 0.97, -1.85, 0.91]\n",
    "assert 5 < dot(beta_0[1:], beta_0[1:]) < 6\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0) < 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eb5e88ce-1920-49ca-9bec-9e6718e0c2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 3246.59it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 3245.60it/s]\n",
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 3257.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#As we increase alpha, the goodness of fit gets worse, but the size of beta gets smaller:\n",
    "\n",
    "beta_0_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 0.1, # alpha\n",
    "                                   learning_rate, 5000, 25)\n",
    "# [30.8, 0.95, -1.83, 0.54]\n",
    "assert 4 < dot(beta_0_1[1:], beta_0_1[1:]) < 5\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_0_1) < 0.69\n",
    "\n",
    "beta_1 = least_squares_fit_ridge(inputs, daily_minutes_good, 1, # alpha\n",
    "                                 learning_rate, 5000, 25)\n",
    "# [30.6, 0.90, -1.68, 0.10]\n",
    "assert 3 < dot(beta_1[1:], beta_1[1:]) < 4\n",
    "assert 0.67 < multiple_r_squared(inputs, daily_minutes_good, beta_1) < 0.69\n",
    "\n",
    "beta_10 = least_squares_fit_ridge(inputs, daily_minutes_good,10, # alpha\n",
    "                                  learning_rate, 5000, 25)\n",
    "# [28.3, 0.67, -0.90, -0.01]\n",
    "assert 1 < dot(beta_10[1:], beta_10[1:]) < 2\n",
    "assert 0.5 < multiple_r_squared(inputs, daily_minutes_good, beta_10) < 0.6\n",
    "\n",
    "#In particular, the coefficient on “PhD” vanishes as we increase the penalty,\n",
    "#which accords with our previous result that it wasn’t significantly different\n",
    "#from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fea02-40fb-4e92-bd12-da118d8d9781",
   "metadata": {},
   "source": [
    "**NOTE** \n",
    "\n",
    "Usually you’d want to rescale your data before using this approach. After all, if you\n",
    "changed years of experience to centuries of experience, its least squares coefficient\n",
    "would increase by a factor of 100 and suddenly get penalized much more, even though\n",
    "it’s the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba0fddf-1837-4080-813f-d3ba33caa4a8",
   "metadata": {},
   "source": [
    "## 2. Lasso Regression\n",
    "\n",
    "- It also uses the penalty.\n",
    "\n",
    "- Whereas the ridge penalty shrank the coefficients overall, the lasso penalty tends to force coefficients to be 0, which makes it good for learning sparse models.\n",
    "\n",
    "- Unfortunately, it’s not amenable to gradient descent, which means that we won’t be able to solve it from scratch.\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "90b8e1e1-fb68-4795-8a2b-5a171dbad58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_penalty(beta, alpha):\n",
    "    return alpha * sum(abs(beta_i) for beta_i in beta[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5b5e3-bd05-4c49-b136-4ccd300d4850",
   "metadata": {},
   "source": [
    "# 9. Regression Using Python Libraries\n",
    "\n",
    "Depending on your use case and requirements, you might choose different libraries:\n",
    "\n",
    ">**scikit-learn** is the most popular for general-purpose regression tasks.  \n",
    ">**Statsmodels** is excellent for statistical analysis and detailed output.  \n",
    ">**TensorFlow** and **PyTorch** are useful for complex or deep learning-based regression.  \n",
    ">**XGBoost** and **LightGBM** are designed for gradient boosting and are suitable for high-performance tasks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "16d6f478-e688-4958-a21e-fc8a1a0ee772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.coef_=array([ 0.        ,  0.91803334, -1.86367139,  1.41052733])\n",
      "model.intercept_=31.185809323516715\n"
     ]
    }
   ],
   "source": [
    "# import lib\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scratch.statistics import daily_minutes_good\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Our data\n",
    "y = daily_minutes_good\n",
    "inputs_x_i = inputs = [[1.,49,4,0],[1,41,9,0],[1,40,8,0],[1,25,6,0],[1,21,1,0],[1,21,0,0],[1,19,3,0],[1,19,0,0],[1,18,9,0],[1,18,8,0],[1,16,4,0],[1,15,3,0],[1,15,0,0],[1,15,2,0],[1,15,7,0],[1,14,0,0],[1,14,1,0],[1,13,1,0],[1,13,7,0],[1,13,4,0],[1,13,2,0],[1,12,5,0],[1,12,0,0],[1,11,9,0],[1,10,9,0],[1,10,1,0],[1,10,1,0],[1,10,7,0],[1,10,9,0],[1,10,1,0],[1,10,6,0],[1,10,6,0],[1,10,8,0],[1,10,10,0],[1,10,6,0],[1,10,0,0],[1,10,5,0],[1,10,3,0],[1,10,4,0],[1,9,9,0],[1,9,9,0],[1,9,0,0],[1,9,0,0],[1,9,6,0],[1,9,10,0],[1,9,8,0],[1,9,5,0],[1,9,2,0],[1,9,9,0],[1,9,10,0],[1,9,7,0],[1,9,2,0],[1,9,0,0],[1,9,4,0],[1,9,6,0],[1,9,4,0],[1,9,7,0],[1,8,3,0],[1,8,2,0],[1,8,4,0],[1,8,9,0],[1,8,2,0],[1,8,3,0],[1,8,5,0],[1,8,8,0],[1,8,0,0],[1,8,9,0],[1,8,10,0],[1,8,5,0],[1,8,5,0],[1,7,5,0],[1,7,5,0],[1,7,0,0],[1,7,2,0],[1,7,8,0],[1,7,10,0],[1,7,5,0],[1,7,3,0],[1,7,3,0],[1,7,6,0],[1,7,7,0],[1,7,7,0],[1,7,9,0],[1,7,3,0],[1,7,8,0],[1,6,4,0],[1,6,6,0],[1,6,4,0],[1,6,9,0],[1,6,0,0],[1,6,1,0],[1,6,4,0],[1,6,1,0],[1,6,0,0],[1,6,7,0],[1,6,0,0],[1,6,8,0],[1,6,4,0],[1,6,2,1],[1,6,1,1],[1,6,3,1],[1,6,6,1],[1,6,4,1],[1,6,4,1],[1,6,1,1],[1,6,3,1],[1,6,4,1],[1,5,1,1],[1,5,9,1],[1,5,4,1],[1,5,6,1],[1,5,4,1],[1,5,4,1],[1,5,10,1],[1,5,5,1],[1,5,2,1],[1,5,4,1],[1,5,4,1],[1,5,9,1],[1,5,3,1],[1,5,10,1],[1,5,2,1],[1,5,2,1],[1,5,9,1],[1,4,8,1],[1,4,6,1],[1,4,0,1],[1,4,10,1],[1,4,5,1],[1,4,10,1],[1,4,9,1],[1,4,1,1],[1,4,4,1],[1,4,4,1],[1,4,0,1],[1,4,3,1],[1,4,1,1],[1,4,3,1],[1,4,2,1],[1,4,4,1],[1,4,4,1],[1,4,8,1],[1,4,2,1],[1,4,4,1],[1,3,2,1],[1,3,6,1],[1,3,4,1],[1,3,7,1],[1,3,4,1],[1,3,1,1],[1,3,10,1],[1,3,3,1],[1,3,4,1],[1,3,7,1],[1,3,5,1],[1,3,6,1],[1,3,1,1],[1,3,6,1],[1,3,10,1],[1,3,2,1],[1,3,4,1],[1,3,2,1],[1,3,1,1],[1,3,5,1],[1,2,4,1],[1,2,2,1],[1,2,8,1],[1,2,3,1],[1,2,1,1],[1,2,9,1],[1,2,10,1],[1,2,9,1],[1,2,4,1],[1,2,5,1],[1,2,0,1],[1,2,9,1],[1,2,9,1],[1,2,0,1],[1,2,1,1],[1,2,1,1],[1,2,4,1],[1,1,0,1],[1,1,2,1],[1,1,2,1],[1,1,5,1],[1,1,3,1],[1,1,10,1],[1,1,6,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,4,1],[1,1,9,1],[1,1,9,1],[1,1,4,1],[1,1,2,1],[1,1,9,1],[1,1,0,1],[1,1,8,1],[1,1,6,1],[1,1,1,1],[1,1,1,1],[1,1,5,1]]\n",
    "\n",
    "# Split data into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs_x_i,y, test_size=0.6, random_state = 42)\n",
    "\n",
    "# Initialize LR model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fitted Coeficients are\n",
    "print(f\"{model.coef_=}\")\n",
    "print(f\"{model.intercept_=}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cb8cf8c6-32b0-45fb-839f-7132d554a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████████████| 5000/5000 [00:01<00:00, 4061.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[31.96564299403721, 0.7803201557555193, -2.014294452395009, 1.2490201223361594]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coeficients from our model\n",
    "least_squares_fit(X_train, y_train, 0.001, 5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "433f784f-bbc2-4e5e-8d25-7bd173dbfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "151836a9-83a6-4d66-85f0-da1dcb354c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.822901360731834\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean square error to evaluate model performance\n",
    "\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0d2d9069-ecb0-49e1-8a05-1b99a7958a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x283b54390>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the regression line\n",
    "\n",
    "plt.scatter([x[0] for x in X_test], y_test, color='blue', label = \"real_test_data\")\n",
    "plt.plot([x[0] for x in X_test], y_pred, color='red', linewidth=3, label = \"predicted_data\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
