{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b200b2b-beb0-4725-9f48-769b89f5c904",
   "metadata": {},
   "source": [
    "# Correlation vs Linear regression\n",
    "\n",
    "- Correlation (-1 to 1) is used to find <u>strength and direction</u> of linear relationship between two variables\n",
    "\n",
    "- In Simple linear regression we <u>quantify and model</u> this linear relationship\n",
    "  - By fitting a straight line to data points\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac679a63-94d4-4d6a-990f-00936c47d095",
   "metadata": {},
   "source": [
    "# The LR Model\n",
    "\n",
    "Assignment: The VP of Engagement asks you to build the model describing the relationship between a DataSciencester user’s <u>number of friends and the amount of time the user spends</u> on the site each day.\n",
    "\n",
    "- we are convinced that its a linear relationship looking at correlation (0.587) - see C5 - Statistics\n",
    "- therefore, let's start with a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fd3072-61a5-4990-912b-29ac8bb3b4f9",
   "metadata": {},
   "source": [
    "## Linear model hypothesis\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = \\beta x_i + \\alpha + \\epsilon_i\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    ">$y_i$ is the number of minutes user i spends on the site daily  \n",
    ">$x_i$ is the number of friends user i has  \n",
    ">$ε_i$ is a (hopefully small) error term representing the fact that there are other factors not accounted for by this simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dcedd-d2f2-4011-9855-bba69edd1ff9",
   "metadata": {},
   "source": [
    "## Steps to train the simple LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2db69-6a97-4171-9de1-d528b4f06e37",
   "metadata": {},
   "source": [
    "### 1. Using normal equations (No iterations) - Least squares fit\n",
    "\n",
    "Find optimized $\\alpha$ and $\\beta$ from given x,y using formula:\n",
    "   \\begin{equation}\n",
    "   \\beta = \\frac{r \\sigma_y}{\\sigma_x}\n",
    "   \\tag{2}\n",
    "   \\end{equation}\n",
    "   >$r$ = correlation(x,y)  \n",
    "   >$\\sigma_y$ = Standard deviation of y  \n",
    "   >$\\sigma_x$ = Standard deviation of x\n",
    "\n",
    "   \\begin{equation}\n",
    "   \\alpha =  \\bar{y} - \\beta \\bar{x}\n",
    "   \\tag{3}\n",
    "   \\end{equation}\n",
    "   > derived from equation (1)\n",
    "\n",
    "- Used when only one predictor ($x$) is present\n",
    "- Its a quick solution\n",
    "- Lower accuracy\n",
    "- Error is not minimized iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04b467-631e-417a-ae78-d11f3ba46458",
   "metadata": {},
   "source": [
    "### 2. Using error minimization through Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c96f4-971b-4a3b-ba59-30cc94453cc8",
   "metadata": {},
   "source": [
    "1. Predict $y_i$ from $ \\alpha , \\beta, x_i$ using,\n",
    "   \\begin{equation}\n",
    "   y_{predicted} = \\beta x_i + \\alpha\n",
    "   \\tag{4}\n",
    "   \\end{equation}\n",
    "\n",
    "2. Calculate error between $y_{predicted}$ and given $y_{i}$ and calculate <u>sum of squared errors</u> (because + and - errors may cancel out each other)\n",
    "   \\begin{equation}\n",
    "   error = y_{predicted} - y_{i}\n",
    "   \\tag{5}\n",
    "   \\end{equation}\n",
    "\n",
    "4. Find R-squared (Coefficient of determination) which measures the fraction of the total variation in the dependent variable that is captured by the model\n",
    "5. Use **gradient descent** - minimize loss function ($error^2$) and get optimum values of $\\alpha$ and $\\beta$ for linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402c2f6-f5b0-478f-aa74-c0afaf78b428",
   "metadata": {},
   "source": [
    "# Using normal equations - Least Squares Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc92c5-a649-4039-82fd-3300cdf8e2f3",
   "metadata": {},
   "source": [
    "(a) **Using calculus the optimized $\\alpha$ and $\\beta$ are given by using equation 2 and 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72571c89-7bd6-4396-9175-7f9e53b18b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from scratch.statistics import correlation, standard_deviation, mean\n",
    "\n",
    "def least_squares_fit(x: List[float], y: List[float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given two vectors x and y,\n",
    "    find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x,y) * standard_deviation(y) /standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334c3e-c5d4-4762-96d4-4640ff52c727",
   "metadata": {},
   "source": [
    "Let's think about why this might be a reasonable solution. \n",
    "\n",
    "> * The choice of $\\alpha$ simply says that -  when we see the average value of the independent variable x, we predict the average value of the dependent variable y.  \n",
    "> * The choice of $\\beta$ means that - when the input value increases by standard_deviation(x), the prediction then increases by `correlation(x, y) * standard_deviation(y)`.  \n",
    "> In the case where x and y are perfectly correlated, a one-standard-deviation increase in x results in a one-standard-deviation-of-y increase in the prediction.    \n",
    "> * When they’re perfectly anticorrelated, the increase in x results in a decrease in the prediction.\n",
    "> * And when the correlation is 0, beta is 0, which means that changes in x don’t affect the prediction at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434dc591-28b4-4bda-94c3-93310a348ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test\n",
    "\n",
    "# get a sample data\n",
    "x = [i for i in range(-100, 110, 10)]\n",
    "y = [3 * i - 5 for i in x]\n",
    "\n",
    "alpha, beta = least_squares_fit(x, y) \n",
    "assert -5.2 < alpha < -4.8\n",
    "assert 2.8 < beta < 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "048997d7-7626-45b7-870f-4f3b56f0d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.752546656812427 0.9322227999047837\n"
     ]
    }
   ],
   "source": [
    "# Let's check with outliers\n",
    "\n",
    "# generate data with outlier\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scratch.statistics import num_friends_good, daily_minutes_good\n",
    "\n",
    "# Now check what's the alpha and beta with outlier data\n",
    "alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)\n",
    "\n",
    "print(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b828f0-eadb-40a0-a842-5c0c067e1902",
   "metadata": {},
   "source": [
    "> - This gives values of alpha = 22.95 and beta = 0.903.\n",
    ">   \n",
    "> - So our model says that we expect a user with n friends to spend 22.95 + n * 0.903 minutes on the site each day.\n",
    ">     \n",
    "> - That is, we predict that a user with no friends on DataSciencester would still spend about 23 minutes a day on the site.\n",
    ">   \n",
    "> - And for each additional friend, we expect a user to spend almost a minute more on the site each day.  \n",
    "\n",
    "\n",
    "\n",
    "**Effect of dataset size on values of $\\alpha$ and $\\beta$:**\n",
    "> - For large number of samples the values of alpha and beta will be optimized to real values\n",
    "> \n",
    "> - But for low number of samples it will be highly deviating\n",
    ">   \n",
    "> - You can run above for less number of data in dataset and see how the alpha and beta are deviated from real values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafaa0ee-af87-43bb-9342-4a347ffd14ac",
   "metadata": {},
   "source": [
    "## R-squared/ Coefficient of Determination\n",
    "\n",
    "-  It is a commonly used metric <u>to evaluate the goodness of fit of a regression model</u>.\n",
    "\n",
    "-  Higher values of $R^2$ indicate a better fit, while lower values indicate a poorer fit.\n",
    "\n",
    "- However, $R^2$ alone may not provide a complete picture of the model's performance, so it's often used in conjunction with other metrics for model evaluation.\n",
    "\n",
    "- It is given by:\n",
    "  $$R^2 =  1 - \\frac{\\sum{(y_p - y)^2}}{\\sum{(y - \\bar{y})^2}} = 1 - \\frac{\\text{sum of sqerrors}(alpha, beta, x, y)}{\\text{total sum of squares}(y)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd6f57-a05e-40d6-b659-ec9d769a6802",
   "metadata": {},
   "source": [
    "**Predict $y$** using $\\alpha , \\beta , x$\n",
    "\n",
    "- Assuming we have determined \\alpha and \\beta, we can make predictions as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f0aa70-fadc-4088-b831-0c4f0ed7fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha: float, beta: float, x_i: float) -> float:\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b9699-fb68-4ece-917c-4b5381637fe0",
   "metadata": {},
   "source": [
    "**Calculate error** from known y value and predicted y value\n",
    "\n",
    "- how do we calculate $\\alpha$ and $\\beta$ here?\n",
    " > Well, any choice of alpha and beta gives us a predicted output for each input x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "699b967d-d220-4dc5-9f5a-cdadd06cb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha: float, beta: float, x_i: float, y_i: float) -> float:\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha\n",
    "    when the actual value is y_i\n",
    "    \"\"\"\n",
    "    return predict(alpha, beta, x_i) - y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06396a-c259-4c91-ba89-7cda6e254792",
   "metadata": {},
   "source": [
    "What we’d really like to know is the **total error over the entire dataset**. But we don’t want to just add the errors—if the prediction for x_1 is too high and the prediction for x_2 is too low, the errors may just cancel out.\n",
    "So instead **we add up the squared errors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81231105-25a3-4193-8579-38876273147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_sqerrors(alpha: float, beta: float, x: List[float], y: List[float]) -> float:\n",
    "    return sum((error(alpha, beta, x_i, y_i) ** 2) for x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84fd7719-d273-41b8-bbe9-317af0b5b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.statistics import de_mean\n",
    "\n",
    "def total_sum_of_squares(y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    the total squared variation of y_i's from their mean\n",
    "    \"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "    \n",
    "def r_squared(alpha: float, beta: float, x: List[float], y: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    the fraction of variation in y captured by the model, which equals\n",
    "    1 - the fraction of variation in y not captured by the model\n",
    "    \"\"\"\n",
    "    return 1.0 - (sum_of_sqerrors(alpha, beta, x, y) / total_sum_of_squares(y))\n",
    "\n",
    "r = r_squared(alpha, beta, num_friends_good, daily_minutes_good)\n",
    "assert 0.328 < r < 0.329"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7f912-4c91-4a57-8353-c20d23c8244a",
   "metadata": {},
   "source": [
    "- for optimized $\\alpha$ and $\\beta$, sum of square errors -> 0\n",
    "\n",
    "- thus $R^2$ -> 1\n",
    "\n",
    "- higher the $R^2$, better the model fits\n",
    "\n",
    "- Here we calculate an R-squared of 0.328, which tells us that our model is only sort of okay at fitting the data, and that <u>clearly there are other factors at play.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d2824-72ac-470a-b46a-c13e9d09940b",
   "metadata": {},
   "source": [
    "# Using Gradient Descent - minimize loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f54f7-be9d-4abf-9a37-e560f1a2297a",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Select random $\\alpha$ and $\\beta$ and a learning rate\n",
    "2. Minimize loss function for optimized $\\alpha$ and $\\beta$\n",
    "3. For this - find gradient of loss function wrt $\\alpha$ and $\\beta$\n",
    "4. Update gradient step size using function gradient_step from scratch.gradient_descent\n",
    "5. Iterate and optimize $\\alpha$, $\\beta$\n",
    "\n",
    "\n",
    "\n",
    "Let's say, $$ \\theta = [\\alpha, \\beta]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080679df-23f0-457f-b845-22680277f48a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "num_epochs = 100000\n",
    "random.seed(0)\n",
    "guess = [random.random(), random.random()] # choose random value to start\n",
    "\n",
    "print(f\"{guess=}\")\n",
    "learning_rate = 0.00001 # set step size\n",
    "\n",
    "with tqdm.trange(num_epochs) as t:\n",
    "    for i in t:\n",
    "        alpha, beta = guess\n",
    "        # print(alpha,beta)\n",
    "        \n",
    "        # Partial derivate of loss function (error^2) wrt alpha\n",
    "        grad_alpha = sum(2 * error(alpha, beta, x_i, y_i)\n",
    "                 for x_i, y_i in zip(num_friends_good,daily_minutes_good))\n",
    "\n",
    "        grad_beta = sum(2 * error(alpha, beta, x_i, y_i) * x_i\n",
    "                        for x_i, y_i in zip(num_friends_good,daily_minutes_good))\n",
    "\n",
    "        # Compute loss to stick in the tqdm description\n",
    "        loss = sum_of_sqerrors(alpha, beta,num_friends_good,daily_minutes_good)\n",
    "        if i % 10 == 0:\n",
    "            t.set_description(f\"alpha: {alpha:.3f}, beta: {beta:.3f}, loss: {loss:.3f}\")\n",
    "        \n",
    "        guess = gradient_step(guess, [grad_alpha, grad_beta], -learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# We should get same result\n",
    "alpha, beta = guess\n",
    "assert 22.94 < alpha < 22.95\n",
    "assert 0.75 < beta < 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "08da8ed2-c7a9-4860-9b53-c33a6bcc7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train and test some random data\n",
    "\n",
    "X = [i for i in range(500)]\n",
    "Y = [3 * i - 5 for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09377505-ffb6-431d-8743-c115e7ccebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch.machine_learning import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "743d4c7b-21bd-4dd1-b090-3acfe046215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "alpha: -4.458, beta: 2.998, loss: 11.316: 100%|█| 100000/100000 [00:06<00:00, 16\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scratch.gradient_descent import gradient_step\n",
    "import tqdm\n",
    "\n",
    "random.seed(0)\n",
    "theta = [-4, 2] # choose random value to start\n",
    "\n",
    "print(theta)\n",
    "learning_rate = 0.00000008 # set step size\n",
    "\n",
    "with tqdm.trange(100000) as t:\n",
    "    for i in t:\n",
    "        alpha, beta = theta\n",
    "        # print(alpha,beta)\n",
    "        \n",
    "        # Partial derivate of loss function (error^2) wrt alpha\n",
    "        grad_alpha = sum(2 * error(alpha, beta, x_i, y_i)\n",
    "                 for x_i, y_i in zip(x_train,y_train))\n",
    "\n",
    "        grad_beta = sum(2 * error(alpha, beta, x_i, y_i) * x_i\n",
    "                        for x_i, y_i in zip(x_train,y_train))\n",
    "\n",
    "        # Compute loss to stick in the tqdm description\n",
    "        loss = sum_of_sqerrors(alpha, beta,x_train,y_train)\n",
    "        if i % 100 == 0:\n",
    "            t.set_description(f\"alpha: {alpha:.3f}, beta: {beta:.3f}, loss: {loss:.3f}\")\n",
    "        \n",
    "        theta = gradient_step(theta, [grad_alpha, grad_beta], -learning_rate)\n",
    "\n",
    "alpha, beta = theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e765ca1-6d1d-499f-a11c-1f0250a3202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999996159390275\n"
     ]
    }
   ],
   "source": [
    "print(r_squared(alpha, beta, x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674e911-1f59-46b4-b5b7-bdab6c883c36",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d31ad-94f3-43c1-89ec-31f7855a8f10",
   "metadata": {},
   "source": [
    "- Imagine you have a bunch of points on a graph, like dots scattered around.\n",
    "  \n",
    "- You want to draw a straight line that goes as close as possible to all these points.\n",
    "  \n",
    "- Now, let's say you have two numbers: one for how steep the line is ($\\beta$) and another for where the line crosses the y-axis ($\\alpha$).\n",
    "  \n",
    "- The least squares method is a way to find the best ($\\alpha$) and ($\\beta$) for your line.\n",
    "\n",
    "**But why do we choose the least squares method?**\n",
    "\n",
    "- Well, imagine you have a bunch of guesses for $\\alpha$ and $\\beta$.\n",
    "  \n",
    "- For each guess, you draw a line and see how far away each point is from the line.\n",
    "  \n",
    "- You square those distances, add them all up, and that's your total \"error\" for that guess.\n",
    "\n",
    "- The least squares method says, \"Let's find the $\\alpha$ and $\\beta$ that make this total 'error' as small as possible.\"\n",
    "\n",
    "- In other words, we're trying to minimize the sum of the squared distances from each point to the line.\n",
    "\n",
    "**The LSM is like maximum likelihood estimation (MLE), why?** \n",
    "\n",
    "- Imagine each point has a tiny cloud of possible positions around it, like a little blob. \n",
    "\n",
    "- We're saying, \"What's the most likely position for the line that makes these blobs overlap the least?\"\n",
    "\n",
    "- That's where the least squares method comes in—it's all about finding the line that makes the observed points the most probable.\n",
    "\n",
    "**So, in simple terms, we choose the least squares method because it's like finding the line that makes the points on our graph the most likely to be where they are. And it turns out, if the errors (the distances from each point to the line) are normally distributed, this method is exactly like maximizing the likelihood of seeing our data.**\n",
    "\n",
    "When we talk about the \"errors\" in a regression model, we mean the differences between the actual values of the dependent variable (the points on our graph) and the values predicted by our regression line. These differences represent how far off our predictions are from the actual data points.\n",
    "\n",
    "Now, imagine plotting all these errors on a graph. If the errors follow a normal distribution, it means they're kind of like a bell curve—most of the errors are small, and fewer errors are really big. This is a common assumption in regression analysis.\n",
    "\n",
    "Now, when we use the least squares method to find the best-fitting line, what we're actually doing is finding the line that minimizes the sum of the squares of these errors. We're making the errors as small as possible.\n",
    "\n",
    "Now, imagine flipping the problem around. Instead of trying to minimize the errors, we're trying to maximize the likelihood of seeing our data. In other words, we're asking, \"What line would make it most likely for us to see these particular data points, given that the errors follow a normal distribution?\"\n",
    "\n",
    "It turns out that these two ways of looking at the problem—the least squares method and maximizing the likelihood of seeing our data—are actually equivalent, as long as the errors are normally distributed. In other words, if the errors follow that bell curve shape, then using the least squares method gives us the same result as if we were trying to maximize the likelihood of seeing our data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
